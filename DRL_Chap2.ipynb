{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/pytorch-drl/blob/main/DRL_Chap2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'PyTorch를 활용한 강화학습/심층강화학습 실전입문' 책 스터디 내용을 Google Colab으로 정리하여 올립니다.\n",
        "\n",
        "Github 예제 코드 주소 : 'https://github.com/wikibook/pytorch-drl'\n",
        "\n",
        "PyTorch를 활용한 강화학습/심층강화학습 실전입문\n",
        "\n",
        "https://wikibook.co.kr/pytorch-drl/"
      ],
      "metadata": {
        "id": "_urokx68V926"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 미로와 에이전트 구현"
      ],
      "metadata": {
        "id": "Z3Xfj4q1aPS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbalEiRqzdfD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# 주피터 노트북 화면에 그림이나 도표를 출력하게 하는 설정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 초기 상태의 미로 모습\n",
        "\n",
        "# 전체 그림의 크기 및 그림을 나타내는 변수 선언\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "ax = plt.gca()\n",
        "\n",
        "# 붉은 벽 그리기 / 붉은 벽은 통과가 불가능함\n",
        "plt.plot([1,1], [0,1], color='red', linewidth=2)\n",
        "plt.plot([1,2], [2,2], color='red', linewidth=2)\n",
        "plt.plot([2,3], [1,1], color='red', linewidth=2)\n",
        "plt.plot([2,2], [1,2], color='red', linewidth=2)\n",
        "\n",
        "# 상태를 의미하는 문자열(S0 ~ S8) 표시\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        "\n",
        "# 그림을 그릴 범위 및 눈금 제거 설정\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
        "\n",
        "# S0에 녹색원으로 현재 위치를 표시\n",
        "line, = ax.plot([0.5], [2.5], marker='o', color='g', markersize=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "y8-jSR00Z_dD",
        "outputId": "de0fc3f8-bcf2-4424-a661-7741e479f123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "강화학습에서 에이전트가 어떻게 행동할지를 결정하는 규칙을 정책(policy)이라고 한다.\n",
        "\n",
        "또한 상태가 $s$일 때, 행동 $a$를 취할 확률은 파라미터 𝛳가 결정하는 정책 π를 따른다는 의미로 $\\pi_{\\theta}(s, a)$로 표기한다.\n",
        "\n",
        "미로탐색에서 상태 $s$는 에이전트의 미로 내 위치에 해당하며, 9개의 상태가 존재한다.\n",
        "\n",
        "행동 $a$는 상태에 있을 떄 에이전트가 취할 수 있는 행동을 나타내며, 상, 하, 좌, 우로 이동하는 4가지 행동을 취할 수 있다. 다만 붉은 벽이 있는 방향으로의 이동은 할 수 없다.\n",
        "\n",
        "정책 $\\pi$ 는 다양한 방법으로 나타낼 수 있다. 함수로 나타낼 수도 있고, 심층강화학습에서는 신경망을 사용하기도 한다.\n",
        "\n",
        "이번장의 태스크인 미로탐색에서는 정책을 타나내는 가장 간단한 방법인 표형식 표현(tabular representation)을 사용할 것이다. 표형식 표현은 행이 상태 $s$, 열이 행동 $a$를 나타낸다."
      ],
      "metadata": {
        "id": "-JhjZUfYgZAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정책 $π$가 함수인 경우 파라미터 $\\theta$는 함수의 변수가 되며, 정책이 신경망인 경우, 유닛 간의 결합 가중치에 해당한다. 표형식 표현으로 정책을 나타내게 되므로 상태 $s$에서 행동$a$를 취할 확률로 변환할 수 있는 값이 된다.\n",
        "\n",
        "먼저 파라미터 $\\theta$의 초깃값 $\\theta_0$을 구현한다. 이동 가능한 방향에는 1, 벽이 있어 이동할 수 없는 방향에는 **np.nan**을 대입했다.\n",
        "\n",
        "열의 순서는 상, 우 하, 좌로 상부터 시계방향순이다."
      ],
      "metadata": {
        "id": "dtOLSDJFCX09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정책을 결정하는 파라미터의 초기값 theta_0 설정\n",
        "\n",
        "# 행은 상태 0~7, 열은 방향(상, 우, 하, 좌)을 나타낸다\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan], # s0\n",
        "                    [np.nan, 1, np.nan, 1], # s1\n",
        "                    [np.nan, np.nan, 1, 1], # s2\n",
        "                    [1, 1, 1, np.nan],      # s3\n",
        "                    [np.nan, np.nan, 1, 1], # s4\n",
        "                    [1, np.nan, np.nan, np.nan], # s5\n",
        "                    [1, np.nan, np.nan, np.nan], # s6\n",
        "                    [1, 1, np.nan, np.nan],     # s7\n",
        "                            ])                            # s8은 목표지점이므로 정책이 없다"
      ],
      "metadata": {
        "id": "LEtnJoBZDrAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```np.nan : Return the sum of array elements over a given axis treating Not a Numbers (NaNs) as zero.```"
      ],
      "metadata": {
        "id": "DX7fvqs6ga0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_0"
      ],
      "metadata": {
        "id": "LhgWPxbBa8cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230faee9-5ad1-4529-8296-333c359d580f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan,  1.,  1., nan],\n",
              "       [nan,  1., nan,  1.],\n",
              "       [nan, nan,  1.,  1.],\n",
              "       [ 1.,  1.,  1., nan],\n",
              "       [nan, nan,  1.,  1.],\n",
              "       [ 1., nan, nan, nan],\n",
              "       [ 1., nan, nan, nan],\n",
              "       [ 1.,  1., nan, nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파라미터를 변환하여 정책을 구하는 방법은 아래와 같다. 단순한 변환 방법을 택해서 이동 방향에 대한 $\\theta$ 값의 비율을 계산해서 확률로 삼는다."
      ],
      "metadata": {
        "id": "ZaUfqzfgF0eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정책 파라미터 theta를 행동 정책 pi로 변환하는 함수\n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "  '''단순히 값의 비율을 계산'''\n",
        "\n",
        "  [m, n] = theta.shape\n",
        "  pi = np.zeros((m, n))\n",
        "  for i in range(0, m):\n",
        "    pi[i, :] = theta[i, :] / np.nansum(theta[i, :]) # 각 행 내에서 취할 수 있는 값 비율\n",
        "\n",
        "  pi = np.nan_to_num(pi) # nan을 0으로 변환\n",
        "\n",
        "  return pi"
      ],
      "metadata": {
        "id": "eHm4FOZpFwvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_0 = simple_convert_into_pi_from_theta(theta_0)\n",
        "pi_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSoyZPG9G9Hl",
        "outputId": "e822a703-78f7-4e41-d5f6-716e753c818a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.5       , 0.5       , 0.        ],\n",
              "       [0.        , 0.5       , 0.        , 0.5       ],\n",
              "       [0.        , 0.        , 0.5       , 0.5       ],\n",
              "       [0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
              "       [0.        , 0.        , 0.5       , 0.5       ],\n",
              "       [1.        , 0.        , 0.        , 0.        ],\n",
              "       [1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.5       , 0.5       , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 에이전트가 정책을 따라 행동하게끔 명령\n",
        "# 1단계 이동 후 에이전트의 상태를 구하는 함수 정의\n",
        "\n",
        "def get_next_s(pi, s):\n",
        "  direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "\n",
        "  next_direction = np.random.choice(direction, p=pi[s, :])\n",
        "  # 현재 상태가 s 일 때, s행의 가능한 행동(방향) a를 랜덤하게 선택\n",
        "\n",
        "  if next_direction == \"up\":\n",
        "    s_next = s - 3 # 위로 이동하려명 상태 값이 3만큼 감소해야한다\n",
        "\n",
        "  elif next_direction == \"right\":\n",
        "    s_next = s + 1\n",
        "\n",
        "  elif next_direction == \"down\":\n",
        "    s_next = s + 3\n",
        "\n",
        "  elif next_direction == \"left\":\n",
        "    s_next = s - 1\n",
        "\n",
        "  return s_next"
      ],
      "metadata": {
        "id": "65_adcQ3HAh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.random.choice\n",
        "- Parameters\n",
        "    a : 1-D array-like or int\n",
        "        If an ndarray, a random sample is generated from its elements.\n",
        "        If an int, the random sample is generated as if it were ``np.arange(a)``\n",
        "    size : int or tuple of ints, optional\n",
        "        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "        ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
        "        single value is returned.\n",
        "    replace : boolean, optional\n",
        "        Whether the sample is with or without replacement. Default is True,\n",
        "        meaning that a value of ``a`` can be selected multiple times.\n",
        "    p : 1-D array-like, optional\n",
        "        The probabilities associated with each entry in a.\n",
        "        If not given, the sample assumes a uniform distribution over all\n",
        "        entries in ``a``."
      ],
      "metadata": {
        "id": "gO3Wz40fbVmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 목표 지점에 이를 때 까지 에이전트를 계속 이동시키는 함수\n",
        "\n",
        "def goal_maze(pi):\n",
        "  s = 0\n",
        "  state_history = [0]\n",
        "\n",
        "  while(1):\n",
        "    next_s = get_next_s(pi, s)\n",
        "    state_history.append(next_s)\n",
        "\n",
        "    if next_s == 8:\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      s = next_s\n",
        "\n",
        "  return state_history"
      ],
      "metadata": {
        "id": "WP3r9zQLIV8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 목표지점에 이를 때까지 미로 안을 이동\n",
        "state_history = goal_maze(pi_0)\n",
        "print(state_history)\n",
        "print(\"목표 지점에 이르기까지 걸린 단계 수는 {} 단계 입니다.\".format(len(state_history)-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovYzQH0bI-Zh",
        "outputId": "5232691b-0f08-4b5c-938f-19577ed1d955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 1, 2, 1, 0, 3, 6, 3, 0, 1, 2, 1, 0, 1, 2, 5, 2, 1, 0, 3, 4, 7, 4, 3, 4, 3, 0, 1, 0, 3, 6, 3, 0, 1, 0, 1, 0, 1, 0, 3, 6, 3, 0, 1, 2, 5, 2, 5, 2, 1, 2, 1, 2, 1, 0, 3, 0, 1, 2, 1, 0, 1, 0, 3, 6, 3, 0, 1, 2, 1, 2, 5, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 5, 2, 1, 2, 5, 2, 1, 2, 1, 0, 1, 2, 5, 2, 5, 2, 1, 0, 1, 2, 5, 2, 1, 0, 1, 2, 5, 2, 5, 2, 1, 0, 1, 2, 5, 2, 1, 0, 3, 6, 3, 6, 3, 4, 3, 4, 7, 8]\n",
            "목표 지점에 이르기까지 걸린 단계 수는 134 단계 입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 에이전트의 이동 과정을 시각화\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def init():\n",
        "  '''배경 이미지 초기화'''\n",
        "  line.set_data([], [])\n",
        "  return (line, )\n",
        "\n",
        "def animate(i):\n",
        "  '''프레임 단위로 이미지 생성'''\n",
        "  state = state_history[i] # 현재 위치\n",
        "  x = (state % 3) + 0.5\n",
        "  y = 2.5 - int(state / 3)\n",
        "  line.set_data([x], [y])\n",
        "  # 교재 내 코드 : line.set_data(x, y)\n",
        "  # MatplotlibDeprecationWarning: Setting data with a non sequence type is deprecated since 3.7 and will be remove two minor releases later\n",
        "  # set_data(x, y) --> set_data([x], [y])\n",
        "  return (line, )\n",
        "\n",
        "# 초기화 함수와 프레임 단위 이미지 생성 함수를 사용해 애니메이션 생성\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(state_history),\n",
        "                               interval = 150, repeat=False) # 전체 그림 fig 에 정의한 함수 animate 사용 + line 이 global variable 임\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "nmghF64oJebK",
        "outputId": "40ed7bb2-6142-41cb-8c88-85ce769b668e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matplotlib.animation.FuncAnimation\n",
        "#matplotlib.animation.FuncAnimation(fig, func, frames=None, init_func=None, fargs=None, save_count=None, *, cache_frame_data=True, **kwargs)[source]"
      ],
      "metadata": {
        "id": "BlfxejdyTamm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function to call at each frame. The first argument will be the next value in frames. Any additional positional arguments can be supplied using functools.partial or via the fargs parameter.\n",
        "\n",
        "The required signature is:\n",
        "\n",
        "```\n",
        "def func(frame, *fargs) -> iterable_of_artists\n",
        "```\n",
        "\n",
        "It is often more convenient to provide the arguments using functools.partial. In this way it is also possible to pass keyword arguments. To pass a function with both positional and keyword arguments, set all arguments as keyword arguments, just leaving the frame argument unset:\n",
        "\n",
        "\n",
        "```\n",
        "def func(frame, art, *, y=None):\n",
        "    ...\n",
        "\n",
        "ani = FuncAnimation(fig, partial(func, art=ln, y='foo'))\n",
        "```\n",
        "\n",
        "\n",
        "If blit == True, func must return an iterable of all artists that were modified or created. This information is used by the blitting algorithm to determine which parts of the figure have to be updated. The return value is unused if blit == False and may be omitted in that case."
      ],
      "metadata": {
        "id": "UNUnRmhrTzAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 정책반복(Policy Gradient) 구현"
      ],
      "metadata": {
        "id": "CBNMOEOYUpjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞절에서 미로 안을 무작위로 이동하는 정책을 구현했다. 사실 확률에 따라 무작위로 왔다갔다하다가 목표에 얻어걸리는 상황이랑 다르지 않았다.\n",
        "\n",
        "이번에는 목표로 곧장 향하도록 정책을 학습시킬 것이다. 크게 두 가지 방법이 있다.\n",
        "\n",
        "첫 번째는 정책에 따라 목표에 빠르게 도달했던 경우에 수행했던 행동(action)을 중요한 것으로 보고, 이때의 행동을 앞으로도 취할 수 있도록 정책을 수정하는 방법이다. 다시 말해 결과가 좋았던 경우에 중시하는 것이다. 딥러닝의 gradient descent와 같은 아이디어로 보인다.\n",
        "\n",
        "두 번째는 목표 지점부터 거슬러 올라가며 목표 지점과 가까운 상태로 에이전트를 유도해 오는 방법이다. 즉 목표 지점 외의 지점에도 가치를 부여하는 것이다."
      ],
      "metadata": {
        "id": "USdRSE8uUtNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째 방법을 정책반복이라고 하며, 두 번째 방법은 가치반복이라고 한다. 이번 절에서는 정책반복 알고리즘 중 하나인 정책경사(policy gradient) 알고리즘을 구현해 보겠다.\n",
        "\n",
        "이번에는 변환함수를 simple_convert_into_pi_from_theta 함수가 아닌, softmax 함수를 사용할 것이다.\n",
        "\n",
        "소프트맥스 함수의 정의는 다음과 같다.\n",
        "\n",
        "$$P(\\theta_i) = \\frac{exp(\\beta\\theta_i)}{\\sum_{j=1}^{N_a}{exp(\\beta\\theta_j)}}$$\n",
        "\n",
        "$\\beta$ 는 역온도 inverse temperature라고 하며 역온도가 작아질수록 행동이 무작위로 선택된다.\n",
        "\n",
        "아래 코드로 확인한 결과 $\\beta$가 커질수록 파라미터 내의 지배적인 값에 치중된 확률이 출력된다.\n",
        "\n",
        "이는 softmax 함수가 아닌 기존의 변환함수에서는 $\\beta$가 곱해진다고 해도 약분이 이루어질수 있었지만, softmax 함수의 사용에서는 $\\beta$가 지수에 곱해지기 때문에, 변수가 동일하게 $e^{\\beta}$만큼 곱해지지 않고 $\\beta$만큼 제곱하게 되어 변수간의 간격이 멀어지게 된다."
      ],
      "metadata": {
        "id": "AjRtUnnoVzoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta_1 = 1\n",
        "beta_2 = 2\n",
        "beta_3 = 4\n",
        "\n",
        "theta_1 = [1,2,3,4,5]\n",
        "theta_2 = np.exp(theta_1)\n",
        "\n",
        "theta_3 = np.exp(np.dot(theta_1,beta_2))\n",
        "\n",
        "theta_4 = np.exp(np.dot(theta_1,beta_3))\n",
        "\n",
        "print(theta_2)\n",
        "print(theta_3)\n",
        "print('\\n', theta_2[0]/np.sum(theta_2), '\\t', theta_3[0]/np.sum(theta_3), '\\t', theta_4[0]/np.sum(theta_4))\n",
        "print('\\n', theta_2[1]/np.sum(theta_2), '\\t', theta_3[1]/np.sum(theta_3), '\\t', theta_4[1]/np.sum(theta_4))\n",
        "print('\\n', theta_2[2]/np.sum(theta_2), '\\t', theta_3[2]/np.sum(theta_3), '\\t', theta_4[2]/np.sum(theta_4))\n",
        "print('\\n', theta_2[3]/np.sum(theta_2), '\\t', theta_3[3]/np.sum(theta_3), '\\t', theta_4[3]/np.sum(theta_4))\n",
        "print('\\n', theta_2[4]/np.sum(theta_2), '\\t', theta_3[4]/np.sum(theta_3), '\\t', theta_4[4]/np.sum(theta_4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajj7eRNgfFjz",
        "outputId": "89b7d637-9422-4ada-d9ef-692a283a2222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591 ]\n",
            "[7.38905610e+00 5.45981500e+01 4.03428793e+02 2.98095799e+03\n",
            " 2.20264658e+04]\n",
            "\n",
            " 0.011656230956039607 \t 0.0002900758675640402 \t 1.1047402132452448e-07\n",
            "\n",
            " 0.03168492079612427 \t 0.0021433868583766707 \t 6.031677191041164e-06\n",
            "\n",
            " 0.0861285444362687 \t 0.015837605738255944 \t 0.0003293184162279595\n",
            "\n",
            " 0.23412165725273662 \t 0.11702495727271915 \t 0.017980176297891573\n",
            "\n",
            " 0.6364086465588308 \t 0.8647039742630842 \t 0.9816843631346681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax_convert_into_pi_from_theta 함수 구현\n",
        "def softmax_convert_into_pi_from_theta(theta):\n",
        "  '''비율 계산에 소프트맥스 함수 사용'''\n",
        "\n",
        "  beta= 1.0\n",
        "  [m, n] = theta.shape\n",
        "  pi = np.zeros((m, n))\n",
        "\n",
        "  exp_theta = np.exp(beta * theta)\n",
        "\n",
        "  for i in range(0, m):\n",
        "    # pi[i, :] = theta[i, :] / np.nansum(theta[i, :])\n",
        "    # 비교 코드\n",
        "    pi[i, :] = exp_theta[i, :] / np.nansum(exp_theta[i, :])\n",
        "\n",
        "  pi = np.nan_to_num(pi)\n",
        "\n",
        "  return pi"
      ],
      "metadata": {
        "id": "tLV7VTGeSkiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 초기 정책 pi_0 계산\n",
        "pi_0 = softmax_convert_into_pi_from_theta(theta_0)\n",
        "print(pi_0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BONDPRqNgom3",
        "outputId": "7102cd48-6e25-455c-f33b-e829bb87d91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.5        0.5        0.        ]\n",
            " [0.         0.5        0.         0.5       ]\n",
            " [0.         0.         0.5        0.5       ]\n",
            " [0.33333333 0.33333333 0.33333333 0.        ]\n",
            " [0.         0.         0.5        0.5       ]\n",
            " [1.         0.         0.         0.        ]\n",
            " [1.         0.         0.         0.        ]\n",
            " [0.5        0.5        0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "초기 상태에서는 두 방법의 결과가 같지만, 학습에 따라 $\\theta$값이 달라지면서 두 방법의 정책값이 서로 크게 달라지게 된다."
      ],
      "metadata": {
        "id": "lUGQuXegmsCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get_next_s 함수에서 수정된 함수\n",
        "# 각 상태에서 취한 행동들 또한 반환하도록 수정\n",
        "\n",
        "def get_action_and_next_s(pi, s):\n",
        "  direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "\n",
        "  next_direction = np.random.choice(direction, p=pi[s, :])\n",
        "  # 현재 상태가 s 일 때, s행의 가능한 행동(방향) a를 랜덤하게 선택\n",
        "\n",
        "  if next_direction == \"up\":\n",
        "    action = 0\n",
        "    s_next = s - 3 # 위로 이동하려명 상태 값이 3만큼 감소해야한다\n",
        "\n",
        "  elif next_direction == \"right\":\n",
        "    action = 1\n",
        "    s_next = s + 1\n",
        "\n",
        "  elif next_direction == \"down\":\n",
        "    action = 2\n",
        "    s_next = s + 3\n",
        "\n",
        "  elif next_direction == \"left\":\n",
        "    action = 3\n",
        "    s_next = s - 1\n",
        "\n",
        "  return [action, s_next]"
      ],
      "metadata": {
        "id": "m_ekX5Q5gvHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# goal_maze 함수에서 수정된 함수\n",
        "# get_action_and_next_s 함수에서 새로 반환한 행동까지 함께 반환하도록 수정\n",
        "\n",
        "def goal_maze_ret_s_a(pi):\n",
        "  s = 0\n",
        "  s_a_history = [[0, np.nan]]\n",
        "\n",
        "  while(1):\n",
        "    # j 시점에서 [action, s_next] : 취할 행동의 종류(방향), j+1 시점의 상태(위치)\n",
        "    [action, next_s] = get_action_and_next_s(pi, s)\n",
        "\n",
        "    # j 시점에서 취하는 행동의 종류 입력\n",
        "    s_a_history[-1][1] = action\n",
        "\n",
        "    # j+1 시점의 상태 미리 입력, j+1 시점에서 취할 행동은 아직 알 수 없으므로 nan으로 입력\n",
        "    s_a_history.append([next_s, np.nan])\n",
        "\n",
        "    if next_s == 8:\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      s = next_s\n",
        "\n",
        "  return s_a_history"
      ],
      "metadata": {
        "id": "bZF84wBAhpKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 초기 정책으로 미로 빠져나오기\n",
        "s_a_history = goal_maze_ret_s_a(pi_0)\n",
        "print(s_a_history)\n",
        "print(\"목표 지점에 이르기까지 걸린 단계 수는 {} 단계 입니다.\".format(len(s_a_history)-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moGXZn-7kFFH",
        "outputId": "de48558c-be0e-404c-e1ed-2efbc5511804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1], [1, 1], [2, 3], [1, 3], [0, 1], [1, 1], [2, 3], [1, 3], [0, 1], [1, 3], [0, 2], [3, 1], [4, 2], [7, 0], [4, 3], [3, 0], [0, 1], [1, 1], [2, 3], [1, 1], [2, 2], [5, 0], [2, 2], [5, 0], [2, 2], [5, 0], [2, 3], [1, 3], [0, 1], [1, 3], [0, 1], [1, 3], [0, 2], [3, 1], [4, 3], [3, 0], [0, 1], [1, 3], [0, 2], [3, 0], [0, 1], [1, 1], [2, 3], [1, 1], [2, 3], [1, 3], [0, 1], [1, 1], [2, 2], [5, 0], [2, 2], [5, 0], [2, 3], [1, 3], [0, 2], [3, 2], [6, 0], [3, 0], [0, 2], [3, 0], [0, 2], [3, 0], [0, 1], [1, 1], [2, 2], [5, 0], [2, 3], [1, 3], [0, 1], [1, 3], [0, 2], [3, 2], [6, 0], [3, 0], [0, 1], [1, 1], [2, 3], [1, 1], [2, 3], [1, 1], [2, 2], [5, 0], [2, 3], [1, 1], [2, 2], [5, 0], [2, 2], [5, 0], [2, 2], [5, 0], [2, 3], [1, 1], [2, 3], [1, 1], [2, 3], [1, 1], [2, 3], [1, 1], [2, 3], [1, 1], [2, 3], [1, 3], [0, 1], [1, 3], [0, 1], [1, 1], [2, 3], [1, 1], [2, 2], [5, 0], [2, 2], [5, 0], [2, 3], [1, 1], [2, 2], [5, 0], [2, 3], [1, 1], [2, 2], [5, 0], [2, 3], [1, 3], [0, 2], [3, 2], [6, 0], [3, 1], [4, 3], [3, 1], [4, 2], [7, 1], [8, nan]]\n",
            "목표 지점에 이르기까지 걸린 단계 수는 130 단계 입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 목표 지점에 도달하였다. 그렇지만 이게 끝이 아니다. 이 과정은 시작에 불과하고 지금부터 앞선 과정에 대한 피드백과 반복이 필요하다.\n",
        "\n",
        "다음으로 정책경사 알고리즘으로 정책을 수정하는 부분을 구현할 차례이다. 정책경사 알고리즘은 아래와 같이 파라미터 $\\theta$ 를 수정한다.\n",
        "\n",
        "$$\\theta_{s_{i}, a_{j}} = \\theta_{s_{i}, a_{j}} + \\eta \\; \\centerdot \\; 𝛥 \\theta_{s, a_{j}} $$\n",
        "\n",
        "$$𝛥 \\theta_{s, a_{j}} = \\{ N(s_{i}, a_{j}) + P(s_{i}, a_{j})N(s_{i}, a)\\}/T$$\n",
        "\n",
        "$N(s_{i}, a_{j})$ : 상태 $s_{i}$에서 $a_{j}$를 취했던 횟수\n",
        "\n",
        "$P(s_{i}, a_{j})$ : 현재 정책하에서 상태 $s_{i}$일 때, 행동 $a_{j}$를 취할 확률을 각각 나타낸다. 이전에 softmax 함수를 써서 구했던 확률값이다.\n",
        "\n",
        "$N(s_{i}, a)$ : 상태 $s_{i}$에서 행동을 취한 횟수의 합계\n",
        "\n",
        "$T$ : 목표 지점에 이르기까지 걸린 모든 단계의 수\n"
      ],
      "metadata": {
        "id": "5YdWNWyBS2zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#theta 수정식\n",
        "\n",
        "def update_theta(theta, pi, s_a_history):\n",
        "  eta = 0.1 #learning_rate\n",
        "  T = len(s_a_history) - 1 #목표지점 제외\n",
        "\n",
        "  [m, n] = theta.shape\n",
        "  delta_theta = theta.copy() #포인터 참조이므로 copy()\n",
        "\n",
        "  #delta_theta 요소 단위로 계산\n",
        "  for i in range(m):\n",
        "    for j in range(n):\n",
        "      if not(np.isnan(theta[i, j])): #nan 이 아닌 경우\n",
        "        SA_i = [SA for SA in s_a_history if SA[0] == i]\n",
        "        # 히스토리에서 상태 i 인 것만 모아놓은 리스트\n",
        "\n",
        "        SA_ij = [SA for SA in s_a_history if SA==[i, j]]\n",
        "        # 히스토리에서 상태 i일 때 행동 j를 취한 경우를 모은 리스트\n",
        "\n",
        "        N_i = len(SA_i)\n",
        "        N_ij = len(SA_ij)\n",
        "\n",
        "        delta_theta[i, j] = (N_i + pi[i, j]*N_ij) / T\n",
        "\n",
        "  new_theta = theta + eta*delta_theta\n",
        "\n",
        "  return new_theta"
      ],
      "metadata": {
        "id": "rI8A1t7SmznU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정책 수정\n",
        "new_theta = update_theta(theta_0, pi_0, s_a_history)\n",
        "pi = softmax_convert_into_pi_from_theta(new_theta)\n",
        "print(pi_0)\n",
        "print(pi)"
      ],
      "metadata": {
        "id": "7Z9x3iQMmzlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71c5bc5-880b-45d9-e0f4-e73f0e30f884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.5        0.5        0.        ]\n",
            " [0.         0.5        0.         0.5       ]\n",
            " [0.         0.         0.5        0.5       ]\n",
            " [0.33333333 0.33333333 0.33333333 0.        ]\n",
            " [0.         0.         0.5        0.5       ]\n",
            " [1.         0.         0.         0.        ]\n",
            " [1.         0.         0.         0.        ]\n",
            " [0.5        0.5        0.         0.        ]]\n",
            "[[0.         0.50057692 0.49942308 0.        ]\n",
            " [0.         0.50067308 0.         0.49932692]\n",
            " [0.         0.         0.49932692 0.50067308]\n",
            " [0.33353279 0.33327633 0.33319088 0.        ]\n",
            " [0.         0.         0.49990385 0.50009615]\n",
            " [1.         0.         0.         0.        ]\n",
            " [1.         0.         0.         0.        ]\n",
            " [0.5        0.5        0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#정책 경사 알고리즘으로 미로 빠져나오기\n",
        "\n",
        "stop_epsilon = 10e-4 #정책의 변화가 이보다 작아지면 학습 종료\n",
        "\n",
        "## 초기값\n",
        "theta = theta_0\n",
        "pi = pi_0\n",
        "\n",
        "is_continue = True\n",
        "count = 1\n",
        "while is_continue:\n",
        "  count += 1\n",
        "  s_a_history = goal_maze_ret_s_a(pi) #정책을 따라 미로를 탐색한 히스토리를 구함\n",
        "  new_theta = update_theta(theta, pi, s_a_history) #파라미터 theta수정\n",
        "  new_pi = softmax_convert_into_pi_from_theta(new_theta) #정책 pi 수정\n",
        "\n",
        "  #print(np.sum(np.abs(new_pi - pi))) #정책 변화 출력\n",
        "  if count%20 == 0:\n",
        "    print(\"count : {} \".format(count), \"목표 지점에 이르기까지 걸린 단계 수는 {}단계 입니다.\".format(len(s_a_history)-1), ' 변화 : {}'.format(np.sum(np.abs(new_pi - pi))))\n",
        "\n",
        "  if np.sum(np.abs(new_pi - pi)) < stop_epsilon:\n",
        "    is_continue = False\n",
        "\n",
        "  else:\n",
        "    theta = new_theta\n",
        "    pi = new_pi\n",
        "\n",
        "print('총 반복 횟수 : {}'.format(count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY5Ox7XzkfdN",
        "outputId": "f7f959e4-77f1-4601-e16f-f1a348a5742b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count : 20  목표 지점에 이르기까지 걸린 단계 수는 44단계 입니다.  변화 : 0.0036651584943059867\n",
            "count : 40  목표 지점에 이르기까지 걸린 단계 수는 4단계 입니다.  변화 : 0.023517068241793004\n",
            "count : 60  목표 지점에 이르기까지 걸린 단계 수는 66단계 입니다.  변화 : 0.007142824338862708\n",
            "count : 80  목표 지점에 이르기까지 걸린 단계 수는 18단계 입니다.  변화 : 0.013099276437964658\n",
            "count : 100  목표 지점에 이르기까지 걸린 단계 수는 72단계 입니다.  변화 : 0.0034698622354124575\n",
            "count : 120  목표 지점에 이르기까지 걸린 단계 수는 22단계 입니다.  변화 : 0.008086398042749021\n",
            "count : 140  목표 지점에 이르기까지 걸린 단계 수는 24단계 입니다.  변화 : 0.008524416870563822\n",
            "count : 160  목표 지점에 이르기까지 걸린 단계 수는 10단계 입니다.  변화 : 0.015026677819541923\n",
            "count : 180  목표 지점에 이르기까지 걸린 단계 수는 14단계 입니다.  변화 : 0.007995064320206191\n",
            "count : 200  목표 지점에 이르기까지 걸린 단계 수는 8단계 입니다.  변화 : 0.014124318178829531\n",
            "count : 220  목표 지점에 이르기까지 걸린 단계 수는 8단계 입니다.  변화 : 0.014911283094633021\n",
            "count : 240  목표 지점에 이르기까지 걸린 단계 수는 4단계 입니다.  변화 : 0.02643416696459333\n",
            "count : 260  목표 지점에 이르기까지 걸린 단계 수는 8단계 입니다.  변화 : 0.014647990541452965\n",
            "count : 280  목표 지점에 이르기까지 걸린 단계 수는 10단계 입니다.  변화 : 0.01246451000484132\n",
            "count : 300  목표 지점에 이르기까지 걸린 단계 수는 4단계 입니다.  변화 : 0.020639242287286697\n",
            "count : 320  목표 지점에 이르기까지 걸린 단계 수는 10단계 입니다.  변화 : 0.010701465653627555\n",
            "count : 340  목표 지점에 이르기까지 걸린 단계 수는 6단계 입니다.  변화 : 0.009514533781574325\n",
            "count : 360  목표 지점에 이르기까지 걸린 단계 수는 6단계 입니다.  변화 : 0.008315770880548988\n",
            "count : 380  목표 지점에 이르기까지 걸린 단계 수는 6단계 입니다.  변화 : 0.005762367081609768\n",
            "count : 400  목표 지점에 이르기까지 걸린 단계 수는 4단계 입니다.  변화 : 0.00630256168879835\n",
            "count : 420  목표 지점에 이르기까지 걸린 단계 수는 4단계 입니다.  변화 : 0.004321851690996863\n",
            "count : 440  목표 지점에 이르기까지 걸린 단계 수는 4단계 입니다.  변화 : 0.002844287940158981\n",
            "count : 460  목표 지점에 이르기까지 걸린 단계 수는 4단계 입니다.  변화 : 0.001803455617543275\n",
            "총 반복 횟수 : 473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 가치반복 알고리즘 관련 용어 정리"
      ],
      "metadata": {
        "id": "ufGMrDGvTGkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 절에서는 보상, 행동가치, 상태가치, 벨만 방정식, 마르코프 결정 프로세스 등의 대해 알아본다.\n",
        "\n",
        "## 보상\n",
        "\n",
        "먼저 가치를 결정하려면 어떤 일정한 척도에 해당하는 개념이 필요하다.\n",
        "\n",
        "강화학습에서는 가치의 척도로 보상(reward)이라는 개념을 사용한다. 미로 태스크를 예로 들면 목표 지점에 도달했을 때 보상을 부여하고, 로봇의 보행 태스크라면 넘어지지 않고 걸어간 거리를 보상으로 삼을 수 있다. 바둑이라면 대국 승리를 보상이라 볼 수 있다.\n",
        "\n",
        "여기서 어떤 시각 $t$에 받을 수 있는 보상 $R_{t}$를 즉각보상(immediate reward)이라고 한다. 그리고 강화학습에서는 보상 $R_{t}$를 태스크에 맞게 적절히 결정해야 한다.\n",
        "\n",
        "그리고 앞으로 받을 수 있으리라 예상되는 보상의 합계 $G_{t}$를 총 보상이라고 한다.\n",
        "\n",
        "$$G_{t} = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots$$\n",
        "\n",
        "다만 이 경우에는 시간의 경과를 고려해야하니 이자율을 포함시켜야 한다. 예를 들어 1만원을 은행에 10년 동안 예치시켜 놓는다면 복리로 이자가 붙어 10년 후에는 액수가 조금 늘어 있을 것이다.\n",
        "\n",
        " 반대로 생각하면 10년 후의 1만원의 가치가 현재의 가치보다 조금 적다고 봐야 한다. 이렇듯 미래의 보상을 할인하는 것을 시간할인(time discount)이라고 하며, 이때의 할인율은 $\\gamma$로 나타낸다.\n",
        "\n",
        " 시간의 경과에 따라 보상합계에 이율 및 복리효과를 적용한 경우에는 할인총보상(dicounted total reward) $G_{t}$를 사용한다.\n",
        "\n",
        " $$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2}R_{t+3} + \\cdots$$\n",
        "\n"
      ],
      "metadata": {
        "id": "IuxeZDJETzKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 행동가치와 상태가치\n",
        "\n",
        "보상을 정의했으니 가치 개념을 도입할 차례이다. 가치반복 알고리즘에는 행동가치(action value)와 상태가치(state value)라는 두 가지 가치를 정의한다.\n",
        "\n",
        "먼저 **행동가치**를 정의한다. 미로 태스크에서 에이전트가 S7에 있다고 가정하자. 이 때 행동 $a = 1$을 하면 목표 지점에 이르러 보상 $R_{t+1}$을 받을 수 있다. 이 내용을 식으로 나타내면 다음과 같다.\n",
        "\n",
        "$$Q^{\\pi}(s=7, a=1) = R_{t+1} = 1$$\n",
        "\n",
        "그럼 에이전트가 행동 $a=0$을 하게 된다면, 목표 지점으로부터 멀어지며 목표에 다다르기 위해서는 S7 → S4 →S7 →S8로  두 단계가 늘어난다, 이를 식으로 나타내면 다음과 같다.\n",
        "\n",
        "$$Q^{\\pi}(s=7, a=0) = R_{t+1} = \\gamma^{2} *1$$\n",
        "\n",
        "이어서 상태가치를 설명한다.\n",
        "\n",
        "상태가치는 상태 $s$에서 정책 $\\pi$를 따라 행동할 때 얻으리라 기대할 수 있는 할인총보상 $G_{t}$를 말한다.\n",
        "\n",
        "예를 들어 에이전트가 S7에서 오른쪽으로 이동하면 목표지점에 도달해서 보상1을 받을 수 있는데, 이를 다음식과 같이 나타낼 수 있다.\n",
        "\n",
        "$$V^{\\pi}(s=7)= 1$$\n",
        "\n",
        "또한 에이전트가 S4에 있을 때 아래로 이동하면 S7에 도착하게 되고 여기서 오른쪽으로 이동하면 목표지점에 도달하므로 상태가치 함수는 다음과 같다.\n",
        "\n",
        "$$V^{pi}(s=4)$$"
      ],
      "metadata": {
        "id": "0iAcIL07TzDz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfTkbfS1TJ79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}