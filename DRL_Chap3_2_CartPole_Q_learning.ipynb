{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/pytorch-drl/blob/main/DRL_Chap3_2_CartPole_Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'PyTorch를 활용한 강화학습/심층강화학습 실전입문' 책 스터디 내용을 Google Colab으로 정리하여 올립니다.\n",
        "\n",
        "Github 예제 코드 주소 : 'https://github.com/wikibook/pytorch-drl'\n",
        "\n",
        "PyTorch를 활용한 강화학습/심층강화학습 실전입문\n",
        "\n",
        "https://wikibook.co.kr/pytorch-drl/"
      ],
      "metadata": {
        "id": "_urokx68V926"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***가상환경 구현 및 함수 수정***"
      ],
      "metadata": {
        "id": "AJTGfJ9a0Spf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matplotlib -downgrade\n",
        "\n",
        "!pip install matplotlib==3.4.2\n",
        "#호출 후 재시작"
      ],
      "metadata": {
        "id": "vt4pA63J0hb4",
        "outputId": "071403d5-78ed-400d-9cd2-123057d9c1c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib==3.4.2 in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (1.25.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.2) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install -y xvfb #server install\n",
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "  !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "  %pip install -U tf-agents pyvirtualdisplay\n",
        "  %pip install -U gym>=0.21.0\n",
        "  %pip install -U gym[box2d,atari, accept-rom-license]\n",
        "\n",
        "!xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook"
      ],
      "metadata": {
        "id": "3laxoCpM0hh7",
        "outputId": "dca757db-5e4e-4613-fb12-a1e48d947d87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.82)] [Connected to cloud.r-pr\u001b[0m\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r                                                                                                    \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [2 InRelease 15.6 kB/119 kB 13%] [Connecting to security.ubuntu.com (91.189.91.82)] [Connected to\u001b[0m\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.82)] [Waiting for headers] [W\u001b[0m\r                                                                                                    \rHit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\u001b[33m\r                                                                                                    \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r                                                                    \r0% [Waiting for headers] [Waiting for headers]\u001b[0m\r                                              \rHit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers]\u001b[0m\r                                              \rHit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers]\u001b[0m\r                                              \rHit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\u001b[33m\r                                              \r0% [Waiting for headers]\u001b[0m\r                        \rHit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Fetched 229 kB in 1s (179 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xorg-dev is already the newest version (1:7.7+23ubuntu2).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "libpq-dev is already the newest version (14.11-0ubuntu0.22.04.1).\n",
            "libsdl2-dev is already the newest version (2.0.20+dfsg-2ubuntu1.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents)\n",
            "  Using cached gym-0.23.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.1.8)\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.26.2\n",
            "    Uninstalling gym-0.26.2:\n",
            "      Successfully uninstalled gym-0.26.2\n",
            "Successfully installed gym-0.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
            "tf-agents 0.19.0 requires gym<=0.23.0,>=0.17.0, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Invalid requirement: 'gym[box2d,atari,'\u001b[0m\u001b[31m\n",
            "\u001b[0m|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json\n",
            "    \t/usr/local/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/usr/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/root/.local/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/root/.jupyter/jupyter_notebook_config.json\n",
            "\n",
            "  _   _          _      _\n",
            " | | | |_ __  __| |__ _| |_ ___\n",
            " | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "  \\___/| .__/\\__,_\\__,_|\\__\\___|\n",
            "       |_|\n",
            "                       \n",
            "Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.\n",
            "\n",
            "https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html\n",
            "\n",
            "Please note that updating to Notebook 7 might break some of your extensions.\n",
            "\n",
            "|INFO|google.colab serverextension initialized.\n",
            "|INFO|Serving notebooks from local directory: /content\n",
            "|INFO|Jupyter Notebook 6.5.5 is running at:\n",
            "|INFO|http://localhost:8888/?token=e5e11fb6eb8ab008c0f7efb1f0d4b3b5e9aa42228ec0c121\n",
            "|INFO| or http://127.0.0.1:8888/?token=e5e11fb6eb8ab008c0f7efb1f0d4b3b5e9aa42228ec0c121\n",
            "|INFO|Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
            "|CRITICAL|\n",
            "    \n",
            "    To access the notebook, open this file in a browser:\n",
            "        file:///root/.local/share/jupyter/runtime/nbserver-92512-open.html\n",
            "    Or copy and paste one of these URLs:\n",
            "        http://localhost:8888/?token=e5e11fb6eb8ab008c0f7efb1f0d4b3b5e9aa42228ec0c121\n",
            "     or http://127.0.0.1:8888/?token=e5e11fb6eb8ab008c0f7efb1f0d4b3b5e9aa42228ec0c121\n",
            "|INFO|interrupted\n",
            "|CRITICAL|Shutting down...\n",
            "|INFO|Shutting down 0 kernels\n",
            "|INFO|Shutting down 0 terminals\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***display_frames_as_gif 함수 정의***"
      ],
      "metadata": {
        "id": "lM2T5YjW0uoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구현에 사용할 패키지 임포트\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gym"
      ],
      "metadata": {
        "id": "4yVtD8BU0881"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 애니메이션을 만드는 함수\n",
        "# 참고 URL : https://github.com/patrickmineault/xcorr-notebooks/blob/master/notebooks/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
        "#!pip install JSAnimation\n",
        "#from JSAnimation.IPython_display import display_animations # error 발생으로 사용하지 않음\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "\n",
        "#새로 정의한 display_animtaion 함수\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_animation(anim):\n",
        "  return HTML(anim.to_jshtml())"
      ],
      "metadata": {
        "id": "qllUOfzc1O8v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_frames_as_gif(frames):\n",
        "  \"\"\"\n",
        "  Displays a list of frames as a gif, with controls\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(frames[0].shape[1]/48.0, frames[0].shape[0]/48.0), dpi=72)\n",
        "  patch = plt.imshow(frames[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=20)\n",
        "\n",
        "  anim.save('movie_cartpole.mp4') #애니메이션을 저장하는 부분\n",
        "  display(display_animation(anim)) #수정된 부분"
      ],
      "metadata": {
        "id": "GCZDAaTS1a3i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import pyvirtualdisplay\n",
        "  display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "\n",
        "except ImportError:\n",
        "  pass"
      ],
      "metadata": {
        "id": "3_wkXdLZiKjA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "obs=env.reset()"
      ],
      "metadata": {
        "id": "DubLks9Gh6bH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_environment(env, figsize=(5, 4)):\n",
        "  plt.figure(figsize=figsize)\n",
        "  img=env.render(mode='rgb_array')\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  return img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "e8i3BpHLhz9-",
        "outputId": "c698a692-2b95-41a5-b779-4cff1d5bed14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "CartPoleEnv.render() got an unexpected keyword argument 'mode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-74e6d5c8ed6d>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplot_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-74e6d5c8ed6d>\u001b[0m in \u001b[0;36mplot_environment\u001b[0;34m(env, figsize)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_render\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_render\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_render_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_render_passive_checker\u001b[0;34m(env, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m             )\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# TODO: Check that the result is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CartPoleEnv.render() got an unexpected keyword argument 'mode'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 다변수, 연속값 상태를 표형식으로 나타내기"
      ],
      "metadata": {
        "id": "Z3Xfj4q1aPS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CartPole 의 상태\n",
        "\n",
        "이전 장에서 다뤘던 미로 태스크에서 상태는 에이전트가 어느 칸에 위치했는지를 변수 하나로 나타냈으며 0~8의 단순 이산값이었다. 그러나 역진자 태스크에서는 더 복잡하게 상태가 정의된다.\n",
        "\n",
        "CartPole 태스크는 observation 변수에 상태를 저장한다. env.step(action)은 게임 환경을 1단계 진행시키는 명령어로 action = 0 일 때 수레를 왼쪽으로 움직이는 것이고 1 일 때 오른쪽으로 움직이는 것에 해당한다.\n",
        "\n",
        "env.step(action) 은 observation, reward, done, info 라는 4개의 변수를 반환한다. observation은 수레와 봉의 상태를 나타내며 상태는 다시 4개의 변수로 이루어져 있다. 이는 수레의 위치 (-2.4 ~ 2.4), 수레의 속도 (-inf ~ inf), 봉의 각도 (-41.8도 ~ 41.8), 봉의 가속도 (-inf ~ inf)이다.\n",
        "\n",
        "reward는 즉각보상이며 action을 실행한 후에 수레의 위치가 $\\pm2.4$ 범위 안에 있고 봉이 $20.9$도 이상 기울어 있지 않다면 보상 1을 받는다. 반대의 경우, 보상은 0이 된다.\n",
        "\n",
        "done 은 게임 종료 여부를 나타내며, 200단계를 초과하거나, 보상이 0일 될 때 게임이 종료되어 done 의 값이 True가 된다.\n",
        "\n",
        "info 는 디버깅 등에 필요한 정보를 담은 변수이다."
      ],
      "metadata": {
        "id": "ia9hsu7NSnFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "observation의 4가지 변수는 모두 연속값으로 이루어져있다. 이를 표형식으로 나타내기 위해 연속값을 이산값으로 변환해야 한다.\n",
        "\n",
        "예를 들어 수레의 위치를 6개의 구간의 이산값으로 변환한다면 -2.4 / -1.6 / -0.8 / 0 / 0.8 / 1.6 / 2.4 로 구간을 나누어 0 ~ 5의 값을 반환할 수 있다. 그러나 이 양 끝 범위를 벗어날 가능성도 있으므로 -Inf ~ -1.6 을 0으로, 1.6 ~ Inf 를 5로 정의한다.\n",
        "\n",
        "다른 변수도 6개의 구간을 갖는 이산변수로 변환한다. 그러면 변수의 가짓수가 $6^{4} = 1296$개로 수레의 상태를 나타낼 수 있다.\n",
        "\n",
        "CartPole 에서 취할 수 있는 행동은 수레를 오른쪽이나 왼쪽으로 밀기 두 가지다. 이후 수레는 가속도를 부여받는다. 각 상태변수를 6개의 구간을 갖는 이산변수로 변환하면 CartPole의 Q함수는 1296행 2열로 된 표로 나타낼 수 있다."
      ],
      "metadata": {
        "id": "rLmhVOJVNMBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 상태의 이산변수 변환 구현\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "ENV = 'CartPole-v0' # 태스크 이름\n",
        "NUM_DIZITIZED = 6 # 각 상태를 이산변수로 변환할 구간 수\n",
        "\n",
        "env = gym.make(ENV) # 실행할 태스크 설정\n",
        "observation = env.reset() # 환경 초기화\n",
        "```\n",
        "\n",
        "env.reset()을 호출하면 초기 상태가 반환되므로 이 반환값을 observation 에 저장한다. 그 다음 이 변수의 값을 이산변수로 변환하는 함수를 정의한다.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# 이산값으로 만들 구간 계산\n",
        "def bins(climp_min, climp_max, num):\n",
        "  '''\n",
        "  관측된 상태(연속값)을 이산값으로 변환하는 구간을 계산\n",
        "  '''\n",
        "  return np.linspace(climp_min, climp_max, num+1)[1:-1]\n",
        "\n",
        "def digitize_state(observation):\n",
        "  '''\n",
        "  관측된 상태(observation 변수)를 이산값으로 변환\n",
        "  '''\n",
        "  cart_pos, cart_v, pole_angle, pole_v = observation\n",
        "\n",
        "  digitized = [\n",
        "    np.digitize(cart_pos, bins=bins(-2.4, 2.4, NUM_DIZITIZED)),\n",
        "    np.digitize(cart_v, bins=bins(-3.0, 3.0, NUM_DIZITIZED)),\n",
        "    np.digitize(pole_angle, bins=bins(-0.5, 0.5, NUM_DIZITIZED)),\n",
        "    np.digitize(pole_v, bins=bins(-2.0, 2.0, NUM_DIZITIZED))]\n",
        "\n",
        "  return sum([x * (NUM_DIGITIZED ** i) for i, x in enumerate(digitized)])\n",
        "    \n",
        "```\n",
        "\n",
        "pole_angle 변수의 구간을 -0.5 ~ 0.5 까지 설정한 이유는 단위가 라디안 radian 이기 때문이다.\n",
        "\n",
        "$0.5rad  = 0.5 \\cdot \\frac{180}{\\pi} \\approx 28.65$  약 29도가 된다.\n",
        "\n",
        "digitize_state 함수의 반환값은 이산값으로 변환한 상태를 나타내는 4개의 변수를 모두 합쳐 0 부터 1295 사이의 값으로 변환한 것이다. NUM_DIGITIZED = 6 이라면 6진수로 계산한다. 만약 digitized = [[1],[2],[3],[4]] 라면 다음과 같이 변환할 수 있다.\n",
        "\n",
        "$$ 1\\cdot 6^{0} + 2\\cdot 6^{1} + 3\\cdot 6^{2} + 4\\cdot 6^{3} = 985$$\n",
        "이는 digitized  = $(\\mathbb{Z}_{6})^{4}$ 와 $(0, 1295) \\subset \\mathbb{Z}$ 두 집합 간의 ***bijection***이 존재하기 때문에 가능하다"
      ],
      "metadata": {
        "id": "pxYB8yMcPNy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum([x*(6**i) for i, x in enumerate([np.array([5]),np.array([5]), np.array([5]), np.array([5])])])"
      ],
      "metadata": {
        "id": "YLbDzq1jTnyn",
        "outputId": "484abf65-5bbf-43da-b53d-3afc3ba7b0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1295])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(enumerate)\n",
        "\n",
        "#  enumerate is useful for obtaining an indexed list:\n",
        "#        (0, seq[0]), (1, seq[1]), (2, seq[2]), ..."
      ],
      "metadata": {
        "id": "N33fCtic1lfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(np.digitize)\n",
        "# digitize(x, bins, right=False)\n",
        "#     Return the indices of the bins to which each value in input array belongs.\n",
        "\n",
        "#     =========  =============  ============================\n",
        "#     `right`    order of bins  returned index `i` satisfies\n",
        "#     =========  =============  ============================\n",
        "#     ``False``  increasing     ``bins[i-1] <= x < bins[i]``\n",
        "#     ``True``   increasing     ``bins[i-1] < x <= bins[i]``\n",
        "#     ``False``  decreasing     ``bins[i-1] > x >= bins[i]``\n",
        "#     ``True``   decreasing     ``bins[i-1] >= x > bins[i]``\n",
        "#     =========  =============  ============================\n",
        "\n",
        "#     If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is\n",
        "#     returned as appropriate."
      ],
      "metadata": {
        "id": "lYekuM9iR4jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV = 'CartPole-v1' # 태스크 이름\n",
        "NUM_DIZITIZED = 6 # 각 상태를 이산변수로 변환할 구간 수\n",
        "\n",
        "env = gym.make(ENV) # 실행할 태스크 설정\n",
        "observation = env.reset() # 환경 초기화"
      ],
      "metadata": {
        "id": "dS9F3iTacqJd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q러닝 구현\n",
        "\n",
        "이번 절에서는 CartPole의 수레를 Q러닝으로 제어할 수 있도록 구현할 것이다. 여기서 정책반복이나 Sarsa 알고리즘 대신 Q러닝을 사용하는 이유는 5장에서 소개할 심층강화학습에서 Q러닝을 사용하기 때문이다. 이번 절에서는 OpenAI Gym의 기존 구현과 맞춰 프로그램을 작성해야한다."
      ],
      "metadata": {
        "id": "Sv-8pmBZcj1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 상수 정의\n",
        "GAMMA = 0.99 # 시간할인율\n",
        "ETA = 0.5 # 학습률\n",
        "MAX_STEPS = 200 # 1에피소드 당 최대 단계 수\n",
        "NUM_EPISODES = 1000 # 최대 에피소드 수"
      ],
      "metadata": {
        "id": "oJAAC0X9ZMie"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  '''CartPole 에이전트 역할을 할 클래스, 봉 달린 수레'''\n",
        "\n",
        "  def __init__(self, num_states, num_actions):\n",
        "    self.brain = Brain(num_states, num_actions) # 에이전트가 행동을 결정하는 두뇌 역할\n",
        "\n",
        "  def update_Q_function(self, observation, action, reward, observation_next):\n",
        "    '''Q함수 수정'''\n",
        "    self.brain.update_Q_table(\n",
        "        observation, action, reward, observation_next)\n",
        "\n",
        "  def get_action(self, observation, step):\n",
        "    '''행동 결정'''\n",
        "    action = self.brain.decide_action(observation, step)\n",
        "    return action"
      ],
      "metadata": {
        "id": "LquZLgZr0Pfs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "교재 코드와 차이점은 두 곳 존재한다.\n",
        "\n",
        "우선 교재내의 Environment 클래스 내 run 메소드의 아래 코드는 다음과 같이 출력된다.\n",
        "\n",
        "\n",
        "```\n",
        "observation = self.env.reset()\n",
        "\n",
        "#(array([-0.0431986 , -0.03948765,  0.0466724 ,  0.03621309], dtype=float32),\n",
        " {})\n",
        "```\n",
        "따라서 다음과 같이 수정을 하였다. 이는 gym 패키지의 버전차이 문제로 보인다.\n",
        "```\n",
        "observation = self.env.reset()[0]\n",
        "```\n",
        "\n",
        "같은 메소드 내에 수정된 코드가 한 줄 더 있다.\n",
        "```\n",
        "observation_next, _, done, _, _ = self.env.step(action)\n",
        "```\n",
        "\n",
        "교재에서는 env.step(action)의 return 값이 4개로 설명되어 있지만\n",
        "```\n",
        "Help on method step in module gym.wrappers.time_limit:\n",
        "\n",
        "step(action) method of gym.wrappers.time_limit.TimeLimit instance\n",
        "    Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\n",
        "    \n",
        "    Args:\n",
        "        action: The environment step action\n",
        "    \n",
        "    Returns:\n",
        "        The environment step ``(observation, reward, terminated, truncated, info)`` with `truncated=True`\n",
        "        if the number of steps elapsed >= max episode steps\n",
        "```\n",
        "truncated 라는 변수가 추가된 것을 볼 수 있다. 이는 gym 클래스 내에 'max_episode_steps'를 지정할 수 있게 되면서 생긴 변수이다. 그러나 해당 교재의 코드는 반복문을 통해 최대 에피소드 관리를 하였고 이 부분을 수정하였다."
      ],
      "metadata": {
        "id": "B0l0Db6WS7D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Brain:\n",
        "  '''에이전트의 두뇌 역할을 하는 클래스, Q러닝을 실제 수행'''\n",
        "\n",
        "  def __init__(self, num_states, num_actions):\n",
        "    self.num_actions = num_actions # 행동의 가짓수(왼쪽, 오른쪽)를 구함\n",
        "\n",
        "    # Q테이블을 생성, 행의 수는 상태를 구간수 ^4 (4는 변수의 수)가지 값 중 하나로 변환한 값, 열의 수는 행동의 가짓수\n",
        "    self.q_table  = np.random.uniform(low=0, high=1, size=(NUM_DIZITIZED**num_states, num_actions))\n",
        "\n",
        "  def bins(self, climp_min, climp_max, num):\n",
        "    '''\n",
        "    관측된 상태(연속값)을 이산값으로 변환하는 구간을 계산\n",
        "    '''\n",
        "    return np.linspace(climp_min, climp_max, num+1)[1:-1]\n",
        "\n",
        "  def digitize_state(self, observation):\n",
        "    '''\n",
        "    관측된 상태(observation 변수)를 이산값으로 변환\n",
        "    '''\n",
        "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
        "\n",
        "    digitized = [\n",
        "      np.digitize(cart_pos, bins=self.bins(-2.4, 2.4, NUM_DIZITIZED)),\n",
        "      np.digitize(cart_v, bins=self.bins(-3.0, 3.0, NUM_DIZITIZED)),\n",
        "      np.digitize(pole_angle, bins=self.bins(-0.5, 0.5, NUM_DIZITIZED)),\n",
        "      np.digitize(pole_v, bins=self.bins(-2.0, 2.0, NUM_DIZITIZED))]\n",
        "\n",
        "    return sum([x * (NUM_DIZITIZED ** i) for i, x in enumerate(digitized)])\n",
        "\n",
        "  def update_Q_table(self, observation, action, reward, observation_next):\n",
        "    '''Q러닝으로 Q테이블을 수정'''\n",
        "    state = self.digitize_state(observation) #상태를 이산변수로 변환\n",
        "    state_next = self.digitize_state(observation_next) #다음 상태를 이산변수로 변환\n",
        "    Max_Q_next = max(self.q_table[state_next][:])\n",
        "    self.q_table[state, action] = self.q_table[state, action] + \\\n",
        "            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
        "\n",
        "  def decide_action(self, observation, episode):\n",
        "    '''ϵ-greedy 알고리즘을 적용해 서서히 최적행동의 비중을 늘림'''\n",
        "    state = self.digitize_state(observation)\n",
        "    epsilon = 0.5 * (1/(episode + 1))\n",
        "\n",
        "    if epsilon <= np.random.uniform(0, 1):\n",
        "      action = np.argmax(self.q_table[state][:])\n",
        "    else:\n",
        "      action = np.random.choice(self.num_actions) # 0, 1 두 가지 행동 중 하나 무작위 선택\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "u3C6r2xf1E-3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "  '''CartPole을 실행하는 환경 역할을 하는 클래스'''\n",
        "\n",
        "  def __init__(self):\n",
        "    self.env = gym.make(ENV)\n",
        "    num_states = self.env.observation_space.shape[0] #태스크의 상태 변수 수를 구함\n",
        "    num_actions = self.env.action_space.n # 가능한 행동 수를 구함\n",
        "    self.agent = Agent(num_states, num_actions) # 에이전트 객체 생성\n",
        "\n",
        "  def run(self):\n",
        "    '''실행'''\n",
        "    complete_episodes = 0 # 성공한 ( 195 단계 이상 버틴 ) 에피소드 수\n",
        "    is_episode_final = False # 마지막 에피소드 여부\n",
        "    frames = [] # 애니메이션 이미지 저장 변수\n",
        "    max_complete_episodes = 0\n",
        "\n",
        "    for episode in range(NUM_EPISODES): # 에피소드 수만큼 반복\n",
        "      observation = self.env.reset()[0] # 환경 초기화\n",
        "      for step in range(MAX_STEPS): # 1 에피소드에 해당하는 반복\n",
        "        if is_episode_final:\n",
        "          frames.append(self.env.render(mode='rgb_array'))\n",
        "\n",
        "        # 행동 선택\n",
        "        action = self.agent.get_action(observation, episode)\n",
        "\n",
        "        # 행동 a_t를 실행해 s_{t+1}, r_{t+1}을 계산\n",
        "        observation_next, _, done, _, _ = self.env.step(action) #reward, info는 사용 x\n",
        "\n",
        "        # 보상을 부여\n",
        "        if done: # 200단계 이상이거나 일정 각도 이상 기울면 done = True\n",
        "          if step < 195:\n",
        "            reward = -1 # 봉이 쓰러지면 페널티 보상 -1 부여\n",
        "            if complete_episodes > max_complete_episodes:\n",
        "              max_complete_episodes = complete_episodes\n",
        "              # 최대 연속 성공 회수 확인\n",
        "            complete_episodes = 0\n",
        "          else:\n",
        "            reward = 1\n",
        "            complete_episodes += 1\n",
        "\n",
        "        else:\n",
        "          reward = 0 # 에피소드 중에는 보상이 0\n",
        "\n",
        "        # 다음 단계의 상태 observation_next로 Q함수 수정\n",
        "        self.agent.update_Q_function(observation, action, reward, observation_next)\n",
        "\n",
        "        # 다음 단계 상태 관측\n",
        "        observation = observation_next\n",
        "\n",
        "        # 에피소드 마무리\n",
        "        if done:\n",
        "          print('{0} Episode: Finished after {1} time steps'. format(\n",
        "              episode, step +1))\n",
        "          if step > 194:\n",
        "            print('***')\n",
        "          break\n",
        "\n",
        "      if is_episode_final: #마지막 에피소드에서 애니메이션을 만들고 저장\n",
        "        display_frames_as_gif(frames)\n",
        "        break\n",
        "\n",
        "      if complete_episodes >= 10:\n",
        "        print('10 에피소드 이상 연속 성공')\n",
        "        is_episode_final = True"
      ],
      "metadata": {
        "id": "VLBDhlxLCskT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "cartpole_env = Environment()\n",
        "cartpole_env.run()"
      ],
      "metadata": {
        "id": "DgiHt4WLMeOP",
        "outputId": "7aed5cb9-620e-4fba-d6ae-be272aaf7f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Episode: Finished after 24 time steps\n",
            "1 Episode: Finished after 10 time steps\n",
            "2 Episode: Finished after 9 time steps\n",
            "3 Episode: Finished after 12 time steps\n",
            "4 Episode: Finished after 14 time steps\n",
            "5 Episode: Finished after 13 time steps\n",
            "6 Episode: Finished after 11 time steps\n",
            "7 Episode: Finished after 10 time steps\n",
            "8 Episode: Finished after 11 time steps\n",
            "9 Episode: Finished after 13 time steps\n",
            "10 Episode: Finished after 10 time steps\n",
            "11 Episode: Finished after 18 time steps\n",
            "12 Episode: Finished after 19 time steps\n",
            "13 Episode: Finished after 18 time steps\n",
            "14 Episode: Finished after 17 time steps\n",
            "15 Episode: Finished after 20 time steps\n",
            "16 Episode: Finished after 15 time steps\n",
            "17 Episode: Finished after 27 time steps\n",
            "18 Episode: Finished after 11 time steps\n",
            "19 Episode: Finished after 32 time steps\n",
            "20 Episode: Finished after 22 time steps\n",
            "21 Episode: Finished after 9 time steps\n",
            "22 Episode: Finished after 12 time steps\n",
            "23 Episode: Finished after 66 time steps\n",
            "24 Episode: Finished after 11 time steps\n",
            "25 Episode: Finished after 23 time steps\n",
            "26 Episode: Finished after 29 time steps\n",
            "27 Episode: Finished after 15 time steps\n",
            "28 Episode: Finished after 57 time steps\n",
            "29 Episode: Finished after 10 time steps\n",
            "30 Episode: Finished after 20 time steps\n",
            "31 Episode: Finished after 10 time steps\n",
            "32 Episode: Finished after 12 time steps\n",
            "33 Episode: Finished after 10 time steps\n",
            "34 Episode: Finished after 10 time steps\n",
            "35 Episode: Finished after 9 time steps\n",
            "36 Episode: Finished after 10 time steps\n",
            "37 Episode: Finished after 17 time steps\n",
            "38 Episode: Finished after 14 time steps\n",
            "39 Episode: Finished after 9 time steps\n",
            "40 Episode: Finished after 16 time steps\n",
            "41 Episode: Finished after 45 time steps\n",
            "42 Episode: Finished after 10 time steps\n",
            "43 Episode: Finished after 39 time steps\n",
            "44 Episode: Finished after 70 time steps\n",
            "45 Episode: Finished after 31 time steps\n",
            "46 Episode: Finished after 44 time steps\n",
            "47 Episode: Finished after 54 time steps\n",
            "48 Episode: Finished after 27 time steps\n",
            "49 Episode: Finished after 17 time steps\n",
            "50 Episode: Finished after 67 time steps\n",
            "51 Episode: Finished after 11 time steps\n",
            "52 Episode: Finished after 10 time steps\n",
            "53 Episode: Finished after 11 time steps\n",
            "54 Episode: Finished after 32 time steps\n",
            "55 Episode: Finished after 34 time steps\n",
            "56 Episode: Finished after 33 time steps\n",
            "57 Episode: Finished after 27 time steps\n",
            "58 Episode: Finished after 15 time steps\n",
            "59 Episode: Finished after 25 time steps\n",
            "60 Episode: Finished after 31 time steps\n",
            "61 Episode: Finished after 29 time steps\n",
            "62 Episode: Finished after 69 time steps\n",
            "63 Episode: Finished after 10 time steps\n",
            "64 Episode: Finished after 12 time steps\n",
            "65 Episode: Finished after 117 time steps\n",
            "66 Episode: Finished after 38 time steps\n",
            "67 Episode: Finished after 66 time steps\n",
            "68 Episode: Finished after 96 time steps\n",
            "69 Episode: Finished after 47 time steps\n",
            "70 Episode: Finished after 100 time steps\n",
            "71 Episode: Finished after 31 time steps\n",
            "72 Episode: Finished after 46 time steps\n",
            "73 Episode: Finished after 51 time steps\n",
            "74 Episode: Finished after 31 time steps\n",
            "75 Episode: Finished after 54 time steps\n",
            "76 Episode: Finished after 49 time steps\n",
            "77 Episode: Finished after 44 time steps\n",
            "78 Episode: Finished after 59 time steps\n",
            "79 Episode: Finished after 8 time steps\n",
            "80 Episode: Finished after 46 time steps\n",
            "81 Episode: Finished after 57 time steps\n",
            "82 Episode: Finished after 24 time steps\n",
            "83 Episode: Finished after 178 time steps\n",
            "84 Episode: Finished after 46 time steps\n",
            "85 Episode: Finished after 82 time steps\n",
            "86 Episode: Finished after 15 time steps\n",
            "87 Episode: Finished after 38 time steps\n",
            "88 Episode: Finished after 48 time steps\n",
            "89 Episode: Finished after 41 time steps\n",
            "90 Episode: Finished after 47 time steps\n",
            "91 Episode: Finished after 22 time steps\n",
            "92 Episode: Finished after 22 time steps\n",
            "93 Episode: Finished after 22 time steps\n",
            "94 Episode: Finished after 20 time steps\n",
            "95 Episode: Finished after 22 time steps\n",
            "96 Episode: Finished after 65 time steps\n",
            "97 Episode: Finished after 18 time steps\n",
            "98 Episode: Finished after 33 time steps\n",
            "99 Episode: Finished after 106 time steps\n",
            "100 Episode: Finished after 110 time steps\n",
            "101 Episode: Finished after 31 time steps\n",
            "102 Episode: Finished after 72 time steps\n",
            "103 Episode: Finished after 27 time steps\n",
            "104 Episode: Finished after 128 time steps\n",
            "107 Episode: Finished after 110 time steps\n",
            "108 Episode: Finished after 76 time steps\n",
            "109 Episode: Finished after 47 time steps\n",
            "110 Episode: Finished after 83 time steps\n",
            "111 Episode: Finished after 131 time steps\n",
            "112 Episode: Finished after 142 time steps\n",
            "115 Episode: Finished after 70 time steps\n",
            "116 Episode: Finished after 89 time steps\n",
            "117 Episode: Finished after 151 time steps\n",
            "119 Episode: Finished after 105 time steps\n",
            "120 Episode: Finished after 81 time steps\n",
            "121 Episode: Finished after 44 time steps\n",
            "122 Episode: Finished after 104 time steps\n",
            "123 Episode: Finished after 95 time steps\n",
            "125 Episode: Finished after 33 time steps\n",
            "126 Episode: Finished after 76 time steps\n",
            "128 Episode: Finished after 101 time steps\n",
            "129 Episode: Finished after 94 time steps\n",
            "130 Episode: Finished after 92 time steps\n",
            "131 Episode: Finished after 126 time steps\n",
            "133 Episode: Finished after 127 time steps\n",
            "134 Episode: Finished after 124 time steps\n",
            "135 Episode: Finished after 72 time steps\n",
            "136 Episode: Finished after 181 time steps\n",
            "137 Episode: Finished after 78 time steps\n",
            "138 Episode: Finished after 12 time steps\n",
            "139 Episode: Finished after 9 time steps\n",
            "140 Episode: Finished after 101 time steps\n",
            "142 Episode: Finished after 144 time steps\n",
            "143 Episode: Finished after 168 time steps\n",
            "144 Episode: Finished after 161 time steps\n",
            "145 Episode: Finished after 128 time steps\n",
            "146 Episode: Finished after 49 time steps\n",
            "147 Episode: Finished after 142 time steps\n",
            "149 Episode: Finished after 196 time steps\n",
            "***\n",
            "150 Episode: Finished after 79 time steps\n",
            "151 Episode: Finished after 157 time steps\n",
            "153 Episode: Finished after 182 time steps\n",
            "154 Episode: Finished after 115 time steps\n",
            "155 Episode: Finished after 188 time steps\n",
            "158 Episode: Finished after 91 time steps\n",
            "159 Episode: Finished after 196 time steps\n",
            "***\n",
            "160 Episode: Finished after 127 time steps\n",
            "162 Episode: Finished after 78 time steps\n",
            "163 Episode: Finished after 120 time steps\n",
            "164 Episode: Finished after 79 time steps\n",
            "165 Episode: Finished after 186 time steps\n",
            "166 Episode: Finished after 73 time steps\n",
            "167 Episode: Finished after 149 time steps\n",
            "168 Episode: Finished after 157 time steps\n",
            "169 Episode: Finished after 77 time steps\n",
            "170 Episode: Finished after 185 time steps\n",
            "171 Episode: Finished after 85 time steps\n",
            "172 Episode: Finished after 43 time steps\n",
            "173 Episode: Finished after 33 time steps\n",
            "174 Episode: Finished after 83 time steps\n",
            "175 Episode: Finished after 148 time steps\n",
            "176 Episode: Finished after 169 time steps\n",
            "177 Episode: Finished after 142 time steps\n",
            "179 Episode: Finished after 160 time steps\n",
            "180 Episode: Finished after 74 time steps\n",
            "182 Episode: Finished after 123 time steps\n",
            "184 Episode: Finished after 171 time steps\n",
            "185 Episode: Finished after 94 time steps\n",
            "187 Episode: Finished after 144 time steps\n",
            "188 Episode: Finished after 163 time steps\n",
            "190 Episode: Finished after 148 time steps\n",
            "191 Episode: Finished after 145 time steps\n",
            "193 Episode: Finished after 163 time steps\n",
            "195 Episode: Finished after 179 time steps\n",
            "196 Episode: Finished after 77 time steps\n",
            "197 Episode: Finished after 134 time steps\n",
            "198 Episode: Finished after 121 time steps\n",
            "200 Episode: Finished after 75 time steps\n",
            "201 Episode: Finished after 130 time steps\n",
            "202 Episode: Finished after 126 time steps\n",
            "203 Episode: Finished after 77 time steps\n",
            "204 Episode: Finished after 185 time steps\n",
            "205 Episode: Finished after 60 time steps\n",
            "206 Episode: Finished after 144 time steps\n",
            "207 Episode: Finished after 157 time steps\n",
            "208 Episode: Finished after 76 time steps\n",
            "209 Episode: Finished after 57 time steps\n",
            "210 Episode: Finished after 103 time steps\n",
            "211 Episode: Finished after 31 time steps\n",
            "212 Episode: Finished after 132 time steps\n",
            "213 Episode: Finished after 125 time steps\n",
            "215 Episode: Finished after 134 time steps\n",
            "216 Episode: Finished after 134 time steps\n",
            "217 Episode: Finished after 61 time steps\n",
            "221 Episode: Finished after 167 time steps\n",
            "222 Episode: Finished after 169 time steps\n",
            "223 Episode: Finished after 117 time steps\n",
            "232 Episode: Finished after 165 time steps\n",
            "235 Episode: Finished after 197 time steps\n",
            "***\n",
            "237 Episode: Finished after 180 time steps\n",
            "238 Episode: Finished after 185 time steps\n",
            "240 Episode: Finished after 185 time steps\n",
            "242 Episode: Finished after 166 time steps\n",
            "244 Episode: Finished after 200 time steps\n",
            "***\n",
            "245 Episode: Finished after 183 time steps\n",
            "252 Episode: Finished after 120 time steps\n",
            "253 Episode: Finished after 169 time steps\n",
            "254 Episode: Finished after 183 time steps\n",
            "255 Episode: Finished after 187 time steps\n",
            "257 Episode: Finished after 197 time steps\n",
            "***\n",
            "260 Episode: Finished after 183 time steps\n",
            "262 Episode: Finished after 196 time steps\n",
            "***\n",
            "269 Episode: Finished after 100 time steps\n",
            "270 Episode: Finished after 99 time steps\n",
            "272 Episode: Finished after 143 time steps\n",
            "275 Episode: Finished after 189 time steps\n",
            "279 Episode: Finished after 177 time steps\n",
            "280 Episode: Finished after 133 time steps\n",
            "283 Episode: Finished after 191 time steps\n",
            "295 Episode: Finished after 137 time steps\n",
            "300 Episode: Finished after 79 time steps\n",
            "301 Episode: Finished after 200 time steps\n",
            "***\n",
            "320 Episode: Finished after 199 time steps\n",
            "***\n",
            "321 Episode: Finished after 145 time steps\n",
            "322 Episode: Finished after 151 time steps\n",
            "323 Episode: Finished after 159 time steps\n",
            "324 Episode: Finished after 146 time steps\n",
            "333 Episode: Finished after 198 time steps\n",
            "***\n",
            "336 Episode: Finished after 59 time steps\n",
            "338 Episode: Finished after 175 time steps\n",
            "341 Episode: Finished after 169 time steps\n",
            "342 Episode: Finished after 72 time steps\n",
            "343 Episode: Finished after 100 time steps\n",
            "344 Episode: Finished after 57 time steps\n",
            "345 Episode: Finished after 16 time steps\n",
            "346 Episode: Finished after 19 time steps\n",
            "347 Episode: Finished after 55 time steps\n",
            "348 Episode: Finished after 99 time steps\n",
            "349 Episode: Finished after 101 time steps\n",
            "350 Episode: Finished after 102 time steps\n",
            "351 Episode: Finished after 151 time steps\n",
            "353 Episode: Finished after 147 time steps\n",
            "355 Episode: Finished after 141 time steps\n",
            "356 Episode: Finished after 181 time steps\n",
            "357 Episode: Finished after 171 time steps\n",
            "358 Episode: Finished after 198 time steps\n",
            "***\n",
            "360 Episode: Finished after 167 time steps\n",
            "361 Episode: Finished after 99 time steps\n",
            "362 Episode: Finished after 159 time steps\n",
            "363 Episode: Finished after 156 time steps\n",
            "364 Episode: Finished after 15 time steps\n",
            "366 Episode: Finished after 155 time steps\n",
            "367 Episode: Finished after 113 time steps\n",
            "369 Episode: Finished after 139 time steps\n",
            "370 Episode: Finished after 157 time steps\n",
            "372 Episode: Finished after 154 time steps\n",
            "374 Episode: Finished after 164 time steps\n",
            "375 Episode: Finished after 31 time steps\n",
            "376 Episode: Finished after 118 time steps\n",
            "378 Episode: Finished after 193 time steps\n",
            "381 Episode: Finished after 152 time steps\n",
            "382 Episode: Finished after 195 time steps\n",
            "383 Episode: Finished after 126 time steps\n",
            "385 Episode: Finished after 155 time steps\n",
            "386 Episode: Finished after 32 time steps\n",
            "388 Episode: Finished after 151 time steps\n",
            "389 Episode: Finished after 184 time steps\n",
            "391 Episode: Finished after 19 time steps\n",
            "392 Episode: Finished after 130 time steps\n",
            "395 Episode: Finished after 88 time steps\n",
            "396 Episode: Finished after 51 time steps\n",
            "397 Episode: Finished after 40 time steps\n",
            "398 Episode: Finished after 25 time steps\n",
            "399 Episode: Finished after 105 time steps\n",
            "400 Episode: Finished after 168 time steps\n",
            "401 Episode: Finished after 139 time steps\n",
            "402 Episode: Finished after 97 time steps\n",
            "403 Episode: Finished after 100 time steps\n",
            "404 Episode: Finished after 159 time steps\n",
            "405 Episode: Finished after 100 time steps\n",
            "406 Episode: Finished after 181 time steps\n",
            "407 Episode: Finished after 182 time steps\n",
            "408 Episode: Finished after 192 time steps\n",
            "409 Episode: Finished after 160 time steps\n",
            "410 Episode: Finished after 33 time steps\n",
            "411 Episode: Finished after 97 time steps\n",
            "412 Episode: Finished after 190 time steps\n",
            "413 Episode: Finished after 118 time steps\n",
            "415 Episode: Finished after 146 time steps\n",
            "416 Episode: Finished after 128 time steps\n",
            "419 Episode: Finished after 15 time steps\n",
            "420 Episode: Finished after 166 time steps\n",
            "421 Episode: Finished after 183 time steps\n",
            "424 Episode: Finished after 16 time steps\n",
            "425 Episode: Finished after 168 time steps\n",
            "426 Episode: Finished after 180 time steps\n",
            "427 Episode: Finished after 96 time steps\n",
            "428 Episode: Finished after 59 time steps\n",
            "430 Episode: Finished after 162 time steps\n",
            "431 Episode: Finished after 167 time steps\n",
            "432 Episode: Finished after 162 time steps\n",
            "435 Episode: Finished after 182 time steps\n",
            "436 Episode: Finished after 196 time steps\n",
            "***\n",
            "437 Episode: Finished after 173 time steps\n",
            "439 Episode: Finished after 30 time steps\n",
            "441 Episode: Finished after 151 time steps\n",
            "442 Episode: Finished after 161 time steps\n",
            "445 Episode: Finished after 63 time steps\n",
            "446 Episode: Finished after 19 time steps\n",
            "447 Episode: Finished after 23 time steps\n",
            "448 Episode: Finished after 105 time steps\n",
            "450 Episode: Finished after 197 time steps\n",
            "***\n",
            "452 Episode: Finished after 17 time steps\n",
            "453 Episode: Finished after 175 time steps\n",
            "454 Episode: Finished after 186 time steps\n",
            "455 Episode: Finished after 179 time steps\n",
            "456 Episode: Finished after 141 time steps\n",
            "457 Episode: Finished after 181 time steps\n",
            "458 Episode: Finished after 161 time steps\n",
            "460 Episode: Finished after 179 time steps\n",
            "461 Episode: Finished after 193 time steps\n",
            "462 Episode: Finished after 188 time steps\n",
            "463 Episode: Finished after 174 time steps\n",
            "464 Episode: Finished after 198 time steps\n",
            "***\n",
            "465 Episode: Finished after 142 time steps\n",
            "466 Episode: Finished after 186 time steps\n",
            "468 Episode: Finished after 176 time steps\n",
            "470 Episode: Finished after 188 time steps\n",
            "471 Episode: Finished after 150 time steps\n",
            "472 Episode: Finished after 167 time steps\n",
            "473 Episode: Finished after 157 time steps\n",
            "474 Episode: Finished after 131 time steps\n",
            "475 Episode: Finished after 153 time steps\n",
            "476 Episode: Finished after 172 time steps\n",
            "477 Episode: Finished after 182 time steps\n",
            "479 Episode: Finished after 195 time steps\n",
            "480 Episode: Finished after 198 time steps\n",
            "***\n",
            "481 Episode: Finished after 195 time steps\n",
            "484 Episode: Finished after 180 time steps\n",
            "485 Episode: Finished after 196 time steps\n",
            "***\n",
            "486 Episode: Finished after 165 time steps\n",
            "488 Episode: Finished after 196 time steps\n",
            "***\n",
            "490 Episode: Finished after 165 time steps\n",
            "492 Episode: Finished after 185 time steps\n",
            "493 Episode: Finished after 155 time steps\n",
            "494 Episode: Finished after 196 time steps\n",
            "***\n",
            "495 Episode: Finished after 177 time steps\n",
            "496 Episode: Finished after 161 time steps\n",
            "498 Episode: Finished after 110 time steps\n",
            "499 Episode: Finished after 134 time steps\n",
            "500 Episode: Finished after 126 time steps\n",
            "501 Episode: Finished after 129 time steps\n",
            "502 Episode: Finished after 172 time steps\n",
            "503 Episode: Finished after 165 time steps\n",
            "506 Episode: Finished after 159 time steps\n",
            "508 Episode: Finished after 141 time steps\n",
            "509 Episode: Finished after 147 time steps\n",
            "510 Episode: Finished after 150 time steps\n",
            "511 Episode: Finished after 199 time steps\n",
            "***\n",
            "512 Episode: Finished after 155 time steps\n",
            "513 Episode: Finished after 145 time steps\n",
            "515 Episode: Finished after 161 time steps\n",
            "516 Episode: Finished after 138 time steps\n",
            "517 Episode: Finished after 154 time steps\n",
            "518 Episode: Finished after 162 time steps\n",
            "521 Episode: Finished after 138 time steps\n",
            "523 Episode: Finished after 160 time steps\n",
            "524 Episode: Finished after 124 time steps\n",
            "526 Episode: Finished after 33 time steps\n",
            "527 Episode: Finished after 182 time steps\n",
            "528 Episode: Finished after 156 time steps\n",
            "529 Episode: Finished after 155 time steps\n",
            "530 Episode: Finished after 177 time steps\n",
            "531 Episode: Finished after 127 time steps\n",
            "534 Episode: Finished after 114 time steps\n",
            "536 Episode: Finished after 171 time steps\n",
            "541 Episode: Finished after 183 time steps\n",
            "543 Episode: Finished after 160 time steps\n",
            "545 Episode: Finished after 175 time steps\n",
            "546 Episode: Finished after 139 time steps\n",
            "548 Episode: Finished after 172 time steps\n",
            "549 Episode: Finished after 179 time steps\n",
            "550 Episode: Finished after 174 time steps\n",
            "551 Episode: Finished after 177 time steps\n",
            "554 Episode: Finished after 193 time steps\n",
            "557 Episode: Finished after 158 time steps\n",
            "560 Episode: Finished after 177 time steps\n",
            "562 Episode: Finished after 166 time steps\n",
            "564 Episode: Finished after 162 time steps\n",
            "566 Episode: Finished after 151 time steps\n",
            "569 Episode: Finished after 199 time steps\n",
            "***\n",
            "571 Episode: Finished after 179 time steps\n",
            "574 Episode: Finished after 193 time steps\n",
            "581 Episode: Finished after 122 time steps\n",
            "592 Episode: Finished after 73 time steps\n",
            "593 Episode: Finished after 46 time steps\n",
            "595 Episode: Finished after 58 time steps\n",
            "596 Episode: Finished after 139 time steps\n",
            "597 Episode: Finished after 124 time steps\n",
            "599 Episode: Finished after 124 time steps\n",
            "600 Episode: Finished after 134 time steps\n",
            "601 Episode: Finished after 139 time steps\n",
            "602 Episode: Finished after 195 time steps\n",
            "603 Episode: Finished after 63 time steps\n",
            "604 Episode: Finished after 38 time steps\n",
            "605 Episode: Finished after 85 time steps\n",
            "606 Episode: Finished after 126 time steps\n",
            "607 Episode: Finished after 66 time steps\n",
            "608 Episode: Finished after 72 time steps\n",
            "609 Episode: Finished after 52 time steps\n",
            "610 Episode: Finished after 197 time steps\n",
            "***\n",
            "611 Episode: Finished after 168 time steps\n",
            "612 Episode: Finished after 169 time steps\n",
            "615 Episode: Finished after 197 time steps\n",
            "***\n",
            "616 Episode: Finished after 192 time steps\n",
            "620 Episode: Finished after 148 time steps\n",
            "621 Episode: Finished after 157 time steps\n",
            "622 Episode: Finished after 161 time steps\n",
            "623 Episode: Finished after 130 time steps\n",
            "624 Episode: Finished after 173 time steps\n",
            "625 Episode: Finished after 153 time steps\n",
            "626 Episode: Finished after 162 time steps\n",
            "627 Episode: Finished after 105 time steps\n",
            "628 Episode: Finished after 109 time steps\n",
            "630 Episode: Finished after 125 time steps\n",
            "631 Episode: Finished after 126 time steps\n",
            "632 Episode: Finished after 117 time steps\n",
            "634 Episode: Finished after 123 time steps\n",
            "635 Episode: Finished after 119 time steps\n",
            "636 Episode: Finished after 144 time steps\n",
            "637 Episode: Finished after 163 time steps\n",
            "638 Episode: Finished after 140 time steps\n",
            "641 Episode: Finished after 179 time steps\n",
            "644 Episode: Finished after 181 time steps\n",
            "645 Episode: Finished after 188 time steps\n",
            "650 Episode: Finished after 174 time steps\n",
            "667 Episode: Finished after 168 time steps\n",
            "669 Episode: Finished after 164 time steps\n",
            "674 Episode: Finished after 198 time steps\n",
            "***\n",
            "675 Episode: Finished after 152 time steps\n",
            "676 Episode: Finished after 133 time steps\n",
            "678 Episode: Finished after 110 time steps\n",
            "679 Episode: Finished after 167 time steps\n",
            "683 Episode: Finished after 184 time steps\n",
            "685 Episode: Finished after 182 time steps\n",
            "695 Episode: Finished after 179 time steps\n",
            "703 Episode: Finished after 200 time steps\n",
            "***\n",
            "707 Episode: Finished after 192 time steps\n",
            "715 Episode: Finished after 198 time steps\n",
            "***\n",
            "716 Episode: Finished after 196 time steps\n",
            "***\n",
            "717 Episode: Finished after 174 time steps\n",
            "720 Episode: Finished after 182 time steps\n",
            "746 Episode: Finished after 186 time steps\n",
            "755 Episode: Finished after 197 time steps\n",
            "***\n",
            "756 Episode: Finished after 167 time steps\n",
            "776 Episode: Finished after 180 time steps\n",
            "783 Episode: Finished after 116 time steps\n",
            "786 Episode: Finished after 168 time steps\n",
            "787 Episode: Finished after 162 time steps\n",
            "791 Episode: Finished after 189 time steps\n",
            "796 Episode: Finished after 135 time steps\n",
            "803 Episode: Finished after 141 time steps\n",
            "805 Episode: Finished after 168 time steps\n",
            "806 Episode: Finished after 185 time steps\n",
            "807 Episode: Finished after 167 time steps\n",
            "809 Episode: Finished after 162 time steps\n",
            "820 Episode: Finished after 109 time steps\n",
            "821 Episode: Finished after 186 time steps\n",
            "825 Episode: Finished after 175 time steps\n",
            "830 Episode: Finished after 185 time steps\n",
            "834 Episode: Finished after 124 time steps\n",
            "836 Episode: Finished after 111 time steps\n",
            "838 Episode: Finished after 157 time steps\n",
            "841 Episode: Finished after 180 time steps\n",
            "853 Episode: Finished after 195 time steps\n",
            "863 Episode: Finished after 195 time steps\n",
            "873 Episode: Finished after 192 time steps\n",
            "892 Episode: Finished after 199 time steps\n",
            "***\n",
            "904 Episode: Finished after 185 time steps\n",
            "916 Episode: Finished after 199 time steps\n",
            "***\n",
            "938 Episode: Finished after 187 time steps\n",
            "943 Episode: Finished after 195 time steps\n",
            "957 Episode: Finished after 195 time steps\n",
            "958 Episode: Finished after 189 time steps\n",
            "976 Episode: Finished after 80 time steps\n",
            "977 Episode: Finished after 59 time steps\n",
            "978 Episode: Finished after 199 time steps\n",
            "***\n",
            "983 Episode: Finished after 176 time steps\n",
            "986 Episode: Finished after 190 time steps\n",
            "996 Episode: Finished after 190 time steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "195 step 이상 성공시 '***' 출력"
      ],
      "metadata": {
        "id": "O_kmjHhPZN_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기준을 160 steps로 낮춘 method run_160 추가 재정의"
      ],
      "metadata": {
        "id": "u3KKz0eSZXLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "  '''CartPole을 실행하는 환경 역할을 하는 클래스'''\n",
        "\n",
        "  def __init__(self):\n",
        "    self.env = gym.make(ENV)\n",
        "    num_states = self.env.observation_space.shape[0] #태스크의 상태 변수 수를 구함\n",
        "    num_actions = self.env.action_space.n # 가능한 행동 수를 구함\n",
        "    self.agent = Agent(num_states, num_actions) # 에이전트 객체 생성\n",
        "\n",
        "  def run(self):\n",
        "    '''실행'''\n",
        "    complete_episodes = 0 # 성공한 ( 195 단계 이상 버틴 ) 에피소드 수\n",
        "    is_episode_final = False # 마지막 에피소드 여부\n",
        "    frames = [] # 애니메이션 이미지 저장 변수\n",
        "    max_complete_episodes = 0\n",
        "\n",
        "    for episode in range(NUM_EPISODES): # 에피소드 수만큼 반복\n",
        "      observation = self.env.reset()[0] # 환경 초기화\n",
        "      for step in range(MAX_STEPS): # 1 에피소드에 해당하는 반복\n",
        "        if is_episode_final:\n",
        "          frames.append(self.env.render(mode='rgb_array'))\n",
        "\n",
        "        # 행동 선택\n",
        "        action = self.agent.get_action(observation, episode)\n",
        "\n",
        "        # 행동 a_t를 실행해 s_{t+1}, r_{t+1}을 계산\n",
        "        observation_next, _, done, _, _ = self.env.step(action) #reward, info는 사용 x\n",
        "\n",
        "        # 보상을 부여\n",
        "        if done: # 200단계 이상이거나 일정 각도 이상 기울면 done = True\n",
        "          if step < 195:\n",
        "            reward = -1 # 봉이 쓰러지면 페널티 보상 -1 부여\n",
        "            if complete_episodes > max_complete_episodes:\n",
        "              max_complete_episodes = complete_episodes\n",
        "              # 최대 연속 성공 회수 확인\n",
        "            complete_episodes = 0\n",
        "          else:\n",
        "            reward = 1\n",
        "            complete_episodes += 1\n",
        "\n",
        "        else:\n",
        "          reward = 0 # 에피소드 중에는 보상이 0\n",
        "\n",
        "        # 다음 단계의 상태 observation_next로 Q함수 수정\n",
        "        self.agent.update_Q_function(observation, action, reward, observation_next)\n",
        "\n",
        "        # 다음 단계 상태 관측\n",
        "        observation = observation_next\n",
        "\n",
        "        # 에피소드 마무리\n",
        "        if done:\n",
        "          print('{0} Episode: Finished after {1} time steps'. format(\n",
        "              episode, step +1))\n",
        "          if step > 194:\n",
        "            print('***')\n",
        "          break\n",
        "\n",
        "      if is_episode_final: #마지막 에피소드에서 애니메이션을 만들고 저장\n",
        "        display_frames_as_gif(frames)\n",
        "        break\n",
        "\n",
        "      if complete_episodes >= 10:\n",
        "        print('10 에피소드 이상 연속 성공')\n",
        "        is_episode_final = True\n",
        "\n",
        "  def run_160(self):\n",
        "    '''실행'''\n",
        "    complete_episodes = 0 # 성공한 ( 195 단계 이상 버틴 ) 에피소드 수\n",
        "    is_episode_final = False # 마지막 에피소드 여부\n",
        "    frames = [] # 애니메이션 이미지 저장 변수\n",
        "    max_complete_episodes = 0\n",
        "\n",
        "    for episode in range(NUM_EPISODES): # 에피소드 수만큼 반복\n",
        "      observation = self.env.reset()[0] # 환경 초기화\n",
        "      for step in range(MAX_STEPS): # 1 에피소드에 해당하는 반복\n",
        "        if is_episode_final:\n",
        "          frames.append(self.env.render(mode='rgb_array'))\n",
        "\n",
        "        # 행동 선택\n",
        "        action = self.agent.get_action(observation, episode)\n",
        "\n",
        "        # 행동 a_t를 실행해 s_{t+1}, r_{t+1}을 계산\n",
        "        observation_next, _, done, _, _ = self.env.step(action) #reward, info는 사용 x\n",
        "\n",
        "        # 보상을 부여\n",
        "        if done: # 200단계 이상이거나 일정 각도 이상 기울면 done = True\n",
        "          if step < 160:\n",
        "            reward = -1 # 봉이 쓰러지면 페널티 보상 -1 부여\n",
        "            if complete_episodes > max_complete_episodes:\n",
        "              max_complete_episodes = complete_episodes\n",
        "              # 최대 연속 성공 회수 확인\n",
        "            complete_episodes = 0\n",
        "          else:\n",
        "            reward = 1\n",
        "            complete_episodes += 1\n",
        "\n",
        "        else:\n",
        "          reward = 0 # 에피소드 중에는 보상이 0\n",
        "\n",
        "        # 다음 단계의 상태 observation_next로 Q함수 수정\n",
        "        self.agent.update_Q_function(observation, action, reward, observation_next)\n",
        "\n",
        "        # 다음 단계 상태 관측\n",
        "        observation = observation_next\n",
        "\n",
        "        # 에피소드 마무리\n",
        "        if done:\n",
        "          print('{0} Episode: Finished after {1} time steps'. format(\n",
        "              episode, step +1))\n",
        "          if step > 159:\n",
        "            print('***')\n",
        "          break\n",
        "\n",
        "      if is_episode_final: #마지막 에피소드에서 애니메이션을 만들고 저장\n",
        "        display_frames_as_gif(frames)\n",
        "        break\n",
        "\n",
        "      if complete_episodes >= 10:\n",
        "        print('10 에피소드 이상 연속 성공')\n",
        "        is_episode_final = True\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "8v1GzcMPZhDj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_env = Environment()\n",
        "cartpole_env.run_160()"
      ],
      "metadata": {
        "id": "muI1jQbLbJki",
        "outputId": "7a6967bf-bc4a-4dce-91d4-30e9f57c21c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Episode: Finished after 13 time steps\n",
            "1 Episode: Finished after 12 time steps\n",
            "2 Episode: Finished after 16 time steps\n",
            "3 Episode: Finished after 23 time steps\n",
            "4 Episode: Finished after 14 time steps\n",
            "5 Episode: Finished after 18 time steps\n",
            "6 Episode: Finished after 18 time steps\n",
            "7 Episode: Finished after 12 time steps\n",
            "8 Episode: Finished after 16 time steps\n",
            "9 Episode: Finished after 26 time steps\n",
            "10 Episode: Finished after 29 time steps\n",
            "11 Episode: Finished after 16 time steps\n",
            "12 Episode: Finished after 80 time steps\n",
            "13 Episode: Finished after 28 time steps\n",
            "14 Episode: Finished after 21 time steps\n",
            "15 Episode: Finished after 18 time steps\n",
            "16 Episode: Finished after 16 time steps\n",
            "17 Episode: Finished after 22 time steps\n",
            "18 Episode: Finished after 18 time steps\n",
            "19 Episode: Finished after 23 time steps\n",
            "20 Episode: Finished after 27 time steps\n",
            "21 Episode: Finished after 47 time steps\n",
            "22 Episode: Finished after 12 time steps\n",
            "23 Episode: Finished after 41 time steps\n",
            "24 Episode: Finished after 12 time steps\n",
            "25 Episode: Finished after 62 time steps\n",
            "26 Episode: Finished after 58 time steps\n",
            "27 Episode: Finished after 165 time steps\n",
            "***\n",
            "28 Episode: Finished after 30 time steps\n",
            "29 Episode: Finished after 12 time steps\n",
            "30 Episode: Finished after 66 time steps\n",
            "31 Episode: Finished after 91 time steps\n",
            "32 Episode: Finished after 11 time steps\n",
            "33 Episode: Finished after 10 time steps\n",
            "34 Episode: Finished after 43 time steps\n",
            "35 Episode: Finished after 65 time steps\n",
            "36 Episode: Finished after 10 time steps\n",
            "37 Episode: Finished after 95 time steps\n",
            "38 Episode: Finished after 62 time steps\n",
            "39 Episode: Finished after 131 time steps\n",
            "40 Episode: Finished after 10 time steps\n",
            "41 Episode: Finished after 65 time steps\n",
            "42 Episode: Finished after 96 time steps\n",
            "43 Episode: Finished after 88 time steps\n",
            "44 Episode: Finished after 118 time steps\n",
            "45 Episode: Finished after 81 time steps\n",
            "46 Episode: Finished after 125 time steps\n",
            "47 Episode: Finished after 165 time steps\n",
            "***\n",
            "48 Episode: Finished after 89 time steps\n",
            "49 Episode: Finished after 10 time steps\n",
            "50 Episode: Finished after 68 time steps\n",
            "51 Episode: Finished after 29 time steps\n",
            "52 Episode: Finished after 71 time steps\n",
            "53 Episode: Finished after 8 time steps\n",
            "54 Episode: Finished after 21 time steps\n",
            "55 Episode: Finished after 10 time steps\n",
            "56 Episode: Finished after 23 time steps\n",
            "57 Episode: Finished after 11 time steps\n",
            "58 Episode: Finished after 144 time steps\n",
            "59 Episode: Finished after 109 time steps\n",
            "60 Episode: Finished after 11 time steps\n",
            "61 Episode: Finished after 8 time steps\n",
            "62 Episode: Finished after 11 time steps\n",
            "63 Episode: Finished after 23 time steps\n",
            "64 Episode: Finished after 10 time steps\n",
            "65 Episode: Finished after 10 time steps\n",
            "66 Episode: Finished after 48 time steps\n",
            "67 Episode: Finished after 150 time steps\n",
            "68 Episode: Finished after 75 time steps\n",
            "69 Episode: Finished after 58 time steps\n",
            "70 Episode: Finished after 70 time steps\n",
            "71 Episode: Finished after 10 time steps\n",
            "72 Episode: Finished after 176 time steps\n",
            "***\n",
            "73 Episode: Finished after 190 time steps\n",
            "***\n",
            "74 Episode: Finished after 105 time steps\n",
            "75 Episode: Finished after 22 time steps\n",
            "76 Episode: Finished after 13 time steps\n",
            "78 Episode: Finished after 112 time steps\n",
            "79 Episode: Finished after 133 time steps\n",
            "80 Episode: Finished after 123 time steps\n",
            "82 Episode: Finished after 162 time steps\n",
            "***\n",
            "85 Episode: Finished after 95 time steps\n",
            "86 Episode: Finished after 50 time steps\n",
            "87 Episode: Finished after 75 time steps\n",
            "88 Episode: Finished after 20 time steps\n",
            "89 Episode: Finished after 136 time steps\n",
            "90 Episode: Finished after 78 time steps\n",
            "91 Episode: Finished after 76 time steps\n",
            "92 Episode: Finished after 32 time steps\n",
            "93 Episode: Finished after 99 time steps\n",
            "94 Episode: Finished after 81 time steps\n",
            "95 Episode: Finished after 62 time steps\n",
            "96 Episode: Finished after 29 time steps\n",
            "97 Episode: Finished after 40 time steps\n",
            "98 Episode: Finished after 113 time steps\n",
            "99 Episode: Finished after 49 time steps\n",
            "100 Episode: Finished after 109 time steps\n",
            "101 Episode: Finished after 68 time steps\n",
            "102 Episode: Finished after 67 time steps\n",
            "103 Episode: Finished after 53 time steps\n",
            "104 Episode: Finished after 122 time steps\n",
            "105 Episode: Finished after 16 time steps\n",
            "106 Episode: Finished after 102 time steps\n",
            "107 Episode: Finished after 135 time steps\n",
            "108 Episode: Finished after 138 time steps\n",
            "109 Episode: Finished after 55 time steps\n",
            "110 Episode: Finished after 102 time steps\n",
            "111 Episode: Finished after 63 time steps\n",
            "112 Episode: Finished after 100 time steps\n",
            "113 Episode: Finished after 107 time steps\n",
            "114 Episode: Finished after 81 time steps\n",
            "115 Episode: Finished after 132 time steps\n",
            "116 Episode: Finished after 134 time steps\n",
            "117 Episode: Finished after 43 time steps\n",
            "118 Episode: Finished after 197 time steps\n",
            "***\n",
            "119 Episode: Finished after 52 time steps\n",
            "120 Episode: Finished after 85 time steps\n",
            "121 Episode: Finished after 49 time steps\n",
            "122 Episode: Finished after 60 time steps\n",
            "123 Episode: Finished after 177 time steps\n",
            "***\n",
            "124 Episode: Finished after 80 time steps\n",
            "125 Episode: Finished after 107 time steps\n",
            "126 Episode: Finished after 51 time steps\n",
            "127 Episode: Finished after 53 time steps\n",
            "128 Episode: Finished after 45 time steps\n",
            "129 Episode: Finished after 133 time steps\n",
            "130 Episode: Finished after 179 time steps\n",
            "***\n",
            "133 Episode: Finished after 47 time steps\n",
            "134 Episode: Finished after 22 time steps\n",
            "135 Episode: Finished after 108 time steps\n",
            "136 Episode: Finished after 72 time steps\n",
            "137 Episode: Finished after 82 time steps\n",
            "138 Episode: Finished after 71 time steps\n",
            "139 Episode: Finished after 90 time steps\n",
            "140 Episode: Finished after 125 time steps\n",
            "141 Episode: Finished after 111 time steps\n",
            "143 Episode: Finished after 98 time steps\n",
            "149 Episode: Finished after 164 time steps\n",
            "***\n",
            "150 Episode: Finished after 64 time steps\n",
            "153 Episode: Finished after 181 time steps\n",
            "***\n",
            "154 Episode: Finished after 194 time steps\n",
            "***\n",
            "156 Episode: Finished after 194 time steps\n",
            "***\n",
            "158 Episode: Finished after 200 time steps\n",
            "***\n",
            "159 Episode: Finished after 162 time steps\n",
            "***\n",
            "161 Episode: Finished after 42 time steps\n",
            "163 Episode: Finished after 186 time steps\n",
            "***\n",
            "164 Episode: Finished after 46 time steps\n",
            "165 Episode: Finished after 144 time steps\n",
            "166 Episode: Finished after 161 time steps\n",
            "***\n",
            "167 Episode: Finished after 172 time steps\n",
            "***\n",
            "168 Episode: Finished after 35 time steps\n",
            "169 Episode: Finished after 96 time steps\n",
            "170 Episode: Finished after 180 time steps\n",
            "***\n",
            "171 Episode: Finished after 195 time steps\n",
            "***\n",
            "173 Episode: Finished after 112 time steps\n",
            "178 Episode: Finished after 53 time steps\n",
            "182 Episode: Finished after 169 time steps\n",
            "***\n",
            "183 Episode: Finished after 105 time steps\n",
            "184 Episode: Finished after 125 time steps\n",
            "185 Episode: Finished after 132 time steps\n",
            "186 Episode: Finished after 140 time steps\n",
            "187 Episode: Finished after 14 time steps\n",
            "188 Episode: Finished after 70 time steps\n",
            "189 Episode: Finished after 104 time steps\n",
            "190 Episode: Finished after 68 time steps\n",
            "192 Episode: Finished after 57 time steps\n",
            "201 Episode: Finished after 113 time steps\n",
            "206 Episode: Finished after 81 time steps\n",
            "208 Episode: Finished after 132 time steps\n",
            "209 Episode: Finished after 161 time steps\n",
            "***\n",
            "211 Episode: Finished after 53 time steps\n",
            "212 Episode: Finished after 114 time steps\n",
            "213 Episode: Finished after 11 time steps\n",
            "215 Episode: Finished after 99 time steps\n",
            "217 Episode: Finished after 11 time steps\n",
            "218 Episode: Finished after 14 time steps\n",
            "219 Episode: Finished after 130 time steps\n",
            "220 Episode: Finished after 43 time steps\n",
            "223 Episode: Finished after 180 time steps\n",
            "***\n",
            "225 Episode: Finished after 150 time steps\n",
            "229 Episode: Finished after 190 time steps\n",
            "***\n",
            "230 Episode: Finished after 111 time steps\n",
            "231 Episode: Finished after 179 time steps\n",
            "***\n",
            "232 Episode: Finished after 168 time steps\n",
            "***\n",
            "233 Episode: Finished after 176 time steps\n",
            "***\n",
            "236 Episode: Finished after 181 time steps\n",
            "***\n",
            "237 Episode: Finished after 109 time steps\n",
            "238 Episode: Finished after 99 time steps\n",
            "240 Episode: Finished after 173 time steps\n",
            "***\n",
            "241 Episode: Finished after 130 time steps\n",
            "242 Episode: Finished after 129 time steps\n",
            "243 Episode: Finished after 153 time steps\n",
            "244 Episode: Finished after 117 time steps\n",
            "245 Episode: Finished after 172 time steps\n",
            "***\n",
            "246 Episode: Finished after 126 time steps\n",
            "247 Episode: Finished after 194 time steps\n",
            "***\n",
            "248 Episode: Finished after 161 time steps\n",
            "***\n",
            "249 Episode: Finished after 109 time steps\n",
            "250 Episode: Finished after 116 time steps\n",
            "251 Episode: Finished after 173 time steps\n",
            "***\n",
            "252 Episode: Finished after 107 time steps\n",
            "253 Episode: Finished after 134 time steps\n",
            "254 Episode: Finished after 174 time steps\n",
            "***\n",
            "255 Episode: Finished after 108 time steps\n",
            "256 Episode: Finished after 99 time steps\n",
            "257 Episode: Finished after 114 time steps\n",
            "258 Episode: Finished after 103 time steps\n",
            "259 Episode: Finished after 119 time steps\n",
            "260 Episode: Finished after 130 time steps\n",
            "261 Episode: Finished after 177 time steps\n",
            "***\n",
            "262 Episode: Finished after 170 time steps\n",
            "***\n",
            "264 Episode: Finished after 109 time steps\n",
            "265 Episode: Finished after 171 time steps\n",
            "***\n",
            "266 Episode: Finished after 71 time steps\n",
            "267 Episode: Finished after 65 time steps\n",
            "268 Episode: Finished after 192 time steps\n",
            "***\n",
            "269 Episode: Finished after 149 time steps\n",
            "270 Episode: Finished after 111 time steps\n",
            "271 Episode: Finished after 104 time steps\n",
            "272 Episode: Finished after 105 time steps\n",
            "273 Episode: Finished after 116 time steps\n",
            "274 Episode: Finished after 89 time steps\n",
            "275 Episode: Finished after 183 time steps\n",
            "***\n",
            "276 Episode: Finished after 171 time steps\n",
            "***\n",
            "277 Episode: Finished after 110 time steps\n",
            "279 Episode: Finished after 129 time steps\n",
            "280 Episode: Finished after 140 time steps\n",
            "282 Episode: Finished after 195 time steps\n",
            "***\n",
            "283 Episode: Finished after 144 time steps\n",
            "284 Episode: Finished after 145 time steps\n",
            "285 Episode: Finished after 174 time steps\n",
            "***\n",
            "286 Episode: Finished after 111 time steps\n",
            "287 Episode: Finished after 113 time steps\n",
            "288 Episode: Finished after 168 time steps\n",
            "***\n",
            "289 Episode: Finished after 148 time steps\n",
            "290 Episode: Finished after 199 time steps\n",
            "***\n",
            "291 Episode: Finished after 165 time steps\n",
            "***\n",
            "292 Episode: Finished after 185 time steps\n",
            "***\n",
            "293 Episode: Finished after 141 time steps\n",
            "294 Episode: Finished after 94 time steps\n",
            "295 Episode: Finished after 143 time steps\n",
            "296 Episode: Finished after 69 time steps\n",
            "297 Episode: Finished after 128 time steps\n",
            "298 Episode: Finished after 83 time steps\n",
            "299 Episode: Finished after 148 time steps\n",
            "300 Episode: Finished after 110 time steps\n",
            "301 Episode: Finished after 80 time steps\n",
            "302 Episode: Finished after 140 time steps\n",
            "303 Episode: Finished after 135 time steps\n",
            "304 Episode: Finished after 128 time steps\n",
            "305 Episode: Finished after 128 time steps\n",
            "306 Episode: Finished after 107 time steps\n",
            "307 Episode: Finished after 83 time steps\n",
            "308 Episode: Finished after 67 time steps\n",
            "309 Episode: Finished after 141 time steps\n",
            "310 Episode: Finished after 102 time steps\n",
            "311 Episode: Finished after 175 time steps\n",
            "***\n",
            "312 Episode: Finished after 92 time steps\n",
            "313 Episode: Finished after 117 time steps\n",
            "314 Episode: Finished after 159 time steps\n",
            "315 Episode: Finished after 92 time steps\n",
            "316 Episode: Finished after 114 time steps\n",
            "317 Episode: Finished after 94 time steps\n",
            "318 Episode: Finished after 156 time steps\n",
            "319 Episode: Finished after 142 time steps\n",
            "320 Episode: Finished after 148 time steps\n",
            "321 Episode: Finished after 154 time steps\n",
            "322 Episode: Finished after 113 time steps\n",
            "323 Episode: Finished after 100 time steps\n",
            "324 Episode: Finished after 152 time steps\n",
            "325 Episode: Finished after 162 time steps\n",
            "***\n",
            "326 Episode: Finished after 144 time steps\n",
            "328 Episode: Finished after 178 time steps\n",
            "***\n",
            "329 Episode: Finished after 171 time steps\n",
            "***\n",
            "330 Episode: Finished after 200 time steps\n",
            "***\n",
            "331 Episode: Finished after 116 time steps\n",
            "332 Episode: Finished after 96 time steps\n",
            "333 Episode: Finished after 142 time steps\n",
            "334 Episode: Finished after 44 time steps\n",
            "335 Episode: Finished after 140 time steps\n",
            "336 Episode: Finished after 152 time steps\n",
            "337 Episode: Finished after 156 time steps\n",
            "338 Episode: Finished after 100 time steps\n",
            "339 Episode: Finished after 156 time steps\n",
            "340 Episode: Finished after 131 time steps\n",
            "341 Episode: Finished after 171 time steps\n",
            "***\n",
            "342 Episode: Finished after 107 time steps\n",
            "343 Episode: Finished after 148 time steps\n",
            "344 Episode: Finished after 179 time steps\n",
            "***\n",
            "345 Episode: Finished after 71 time steps\n",
            "346 Episode: Finished after 87 time steps\n",
            "347 Episode: Finished after 87 time steps\n",
            "348 Episode: Finished after 111 time steps\n",
            "349 Episode: Finished after 63 time steps\n",
            "351 Episode: Finished after 99 time steps\n",
            "352 Episode: Finished after 94 time steps\n",
            "353 Episode: Finished after 92 time steps\n",
            "354 Episode: Finished after 142 time steps\n",
            "355 Episode: Finished after 136 time steps\n",
            "356 Episode: Finished after 170 time steps\n",
            "***\n",
            "357 Episode: Finished after 95 time steps\n",
            "358 Episode: Finished after 178 time steps\n",
            "***\n",
            "359 Episode: Finished after 135 time steps\n",
            "360 Episode: Finished after 142 time steps\n",
            "361 Episode: Finished after 98 time steps\n",
            "364 Episode: Finished after 109 time steps\n",
            "365 Episode: Finished after 187 time steps\n",
            "***\n",
            "366 Episode: Finished after 149 time steps\n",
            "367 Episode: Finished after 107 time steps\n",
            "368 Episode: Finished after 145 time steps\n",
            "369 Episode: Finished after 152 time steps\n",
            "370 Episode: Finished after 80 time steps\n",
            "371 Episode: Finished after 173 time steps\n",
            "***\n",
            "372 Episode: Finished after 121 time steps\n",
            "373 Episode: Finished after 110 time steps\n",
            "374 Episode: Finished after 65 time steps\n",
            "375 Episode: Finished after 66 time steps\n",
            "376 Episode: Finished after 10 time steps\n",
            "377 Episode: Finished after 98 time steps\n",
            "378 Episode: Finished after 18 time steps\n",
            "379 Episode: Finished after 106 time steps\n",
            "382 Episode: Finished after 161 time steps\n",
            "***\n",
            "391 Episode: Finished after 175 time steps\n",
            "***\n",
            "397 Episode: Finished after 195 time steps\n",
            "***\n",
            "402 Episode: Finished after 189 time steps\n",
            "***\n",
            "403 Episode: Finished after 181 time steps\n",
            "***\n",
            "405 Episode: Finished after 180 time steps\n",
            "***\n",
            "410 Episode: Finished after 183 time steps\n",
            "***\n",
            "416 Episode: Finished after 190 time steps\n",
            "***\n",
            "430 Episode: Finished after 50 time steps\n",
            "432 Episode: Finished after 185 time steps\n",
            "***\n",
            "433 Episode: Finished after 182 time steps\n",
            "***\n",
            "437 Episode: Finished after 166 time steps\n",
            "***\n",
            "438 Episode: Finished after 37 time steps\n",
            "439 Episode: Finished after 130 time steps\n",
            "440 Episode: Finished after 59 time steps\n",
            "442 Episode: Finished after 116 time steps\n",
            "451 Episode: Finished after 165 time steps\n",
            "***\n",
            "459 Episode: Finished after 168 time steps\n",
            "***\n",
            "461 Episode: Finished after 168 time steps\n",
            "***\n",
            "462 Episode: Finished after 166 time steps\n",
            "***\n",
            "464 Episode: Finished after 192 time steps\n",
            "***\n",
            "466 Episode: Finished after 142 time steps\n",
            "467 Episode: Finished after 198 time steps\n",
            "***\n",
            "469 Episode: Finished after 148 time steps\n",
            "470 Episode: Finished after 185 time steps\n",
            "***\n",
            "472 Episode: Finished after 156 time steps\n",
            "473 Episode: Finished after 156 time steps\n",
            "474 Episode: Finished after 130 time steps\n",
            "475 Episode: Finished after 157 time steps\n",
            "476 Episode: Finished after 189 time steps\n",
            "***\n",
            "480 Episode: Finished after 183 time steps\n",
            "***\n",
            "483 Episode: Finished after 155 time steps\n",
            "486 Episode: Finished after 186 time steps\n",
            "***\n",
            "489 Episode: Finished after 172 time steps\n",
            "***\n",
            "491 Episode: Finished after 191 time steps\n",
            "***\n",
            "494 Episode: Finished after 161 time steps\n",
            "***\n",
            "497 Episode: Finished after 189 time steps\n",
            "***\n",
            "498 Episode: Finished after 178 time steps\n",
            "***\n",
            "499 Episode: Finished after 182 time steps\n",
            "***\n",
            "500 Episode: Finished after 166 time steps\n",
            "***\n",
            "501 Episode: Finished after 189 time steps\n",
            "***\n",
            "502 Episode: Finished after 187 time steps\n",
            "***\n",
            "10 에피소드 이상 연속 성공\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "CartPoleEnv.render() got an unexpected keyword argument 'mode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-521b900a9917>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcartpole_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcartpole_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_160\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-eb4602953c56>\u001b[0m in \u001b[0;36mrun_160\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 1 에피소드에 해당하는 반복\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_episode_final\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m           \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# 행동 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_render\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_render\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_render_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_render_passive_checker\u001b[0;34m(env, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m             )\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# TODO: Check that the result is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CartPoleEnv.render() got an unexpected keyword argument 'mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ϵ$ ϵ\n"
      ],
      "metadata": {
        "id": "wc-51fWhCKUF"
      }
    }
  ]
}