{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME330V9v4Fa4yG7z0+zzM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/pytorch-drl/blob/main/DRL_Chap5_Deep_Q_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***가상환경 구현 및 함수 수정***"
      ],
      "metadata": {
        "id": "AJTGfJ9a0Spf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matplotlib -downgrade\n",
        "\n",
        "!pip install matplotlib==3.4.2\n",
        "#호출 후 재시작"
      ],
      "metadata": {
        "id": "vt4pA63J0hb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0da49ea-9668-487d-f5cc-61aa9b063792"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib==3.4.2 in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (1.25.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.4.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.2) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install -y xvfb #server install\n",
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "  !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "  %pip install -U tf-agents pyvirtualdisplay\n",
        "  %pip install -U gym>=0.21.0\n",
        "  %pip install -U gym[box2d,atari, accept-rom-license]\n",
        "\n",
        "!xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook"
      ],
      "metadata": {
        "id": "3laxoCpM0hh7",
        "outputId": "630ec713-4a8f-4fc6-8591-ffbeed23c662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ubuntu.com (91.189.91.8\u001b[0m\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r                                                                                                    \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r                                                                    \r0% [Waiting for headers] [Waiting for headers]\u001b[0m\r                                              \rHit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "\u001b[33m\r                                              \r0% [Waiting for headers]\u001b[0m\r                        \rHit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 119 kB in 2s (73.7 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xorg-dev is already the newest version (1:7.7+23ubuntu2).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "libpq-dev is already the newest version (14.11-0ubuntu0.22.04.1).\n",
            "libsdl2-dev is already the newest version (2.0.20+dfsg-2ubuntu1.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents)\n",
            "  Using cached gym-0.23.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.1.8)\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.26.2\n",
            "    Uninstalling gym-0.26.2:\n",
            "      Successfully uninstalled gym-0.26.2\n",
            "Successfully installed gym-0.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              },
              "id": "8c0f4cad96a64709a7c109d04163a403"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Invalid requirement: 'gym[box2d,atari,'\u001b[0m\u001b[31m\n",
            "\u001b[0m|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json\n",
            "    \t/usr/local/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/usr/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/root/.local/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/root/.jupyter/jupyter_notebook_config.json\n",
            "\n",
            "  _   _          _      _\n",
            " | | | |_ __  __| |__ _| |_ ___\n",
            " | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "  \\___/| .__/\\__,_\\__,_|\\__\\___|\n",
            "       |_|\n",
            "                       \n",
            "Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.\n",
            "\n",
            "https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html\n",
            "\n",
            "Please note that updating to Notebook 7 might break some of your extensions.\n",
            "\n",
            "|INFO|google.colab serverextension initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***display_frames_as_gif 함수 정의***"
      ],
      "metadata": {
        "id": "lM2T5YjW0uoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구현에 사용할 패키지 임포트\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gym"
      ],
      "metadata": {
        "id": "4yVtD8BU0881"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 애니메이션을 만드는 함수\n",
        "# 참고 URL : https://github.com/patrickmineault/xcorr-notebooks/blob/master/notebooks/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
        "#!pip install JSAnimation\n",
        "#from JSAnimation.IPython_display import display_animations # error 발생으로 사용하지 않음\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "\n",
        "#새로 정의한 display_animtaion 함수\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "qllUOfzc1O8v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_animation(anim):\n",
        "  return HTML(anim.to_jshtml())"
      ],
      "metadata": {
        "id": "7ap2-wQnq3Uq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_frames_as_gif(frames):\n",
        "  \"\"\"\n",
        "  Displays a list of frames as a gif, with controls\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(frames[0].shape[1]/48.0, frames[0].shape[0]/48.0), dpi=72)\n",
        "  patch = plt.imshow(frames[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=20)\n",
        "\n",
        "  anim.save('movie_cartpole.mp4') #애니메이션을 저장하는 부분\n",
        "  display(display_animation(anim)) #수정된 부분"
      ],
      "metadata": {
        "id": "GCZDAaTS1a3i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import pyvirtualdisplay\n",
        "  display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "\n",
        "except ImportError:\n",
        "  pass"
      ],
      "metadata": {
        "id": "3_wkXdLZiKjA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "obs=env.reset()"
      ],
      "metadata": {
        "id": "DubLks9Gh6bH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_environment(env, figsize=(5, 4)):\n",
        "  plt.figure(figsize=figsize)\n",
        "  img=env.render(mode='rgb_array')\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  return img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e8i3BpHLhz9-",
        "outputId": "e5afe725-7e1a-43c6-b3a0-915fa0138cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. 딥러닝을 적용한 Q 러닝\n",
        "\n",
        "### 표형식 표현의 문제점\n",
        "\n",
        "표형식 표현을 사용하는 $Q$러닝의 문제점은 상태변수의 종류가 늘어나거나 각 변수를 이산변수로 변환할 때 구간을 세세하게 분할하거나 하면 표의 행 수가 크게 늘어난다는 점이다. 예를 들어 이미지를 상태로 받는다면 상태변수는 이미지의 각 픽셀이 될 것이다.\n",
        "\n",
        "행 수가 많은 표형식 표현으로 제대로 강화학습을 진행하려면 에피소드 수가 매우 많이 필요하다. 이 때문에 상태 수가 많은 태스크는 표형식 표현으로는 제대로 학습시키기가 현실적으로 어렵다.\n",
        "\n",
        "딥러닝은 이러한 ***상태변수 수가 많아지면 표형식 표현으로는 강화학습이 어렵다***는 문제를 해결하기 위한 것이다.\n",
        "\n",
        "### 심층강화학습 알고리즘 DQN\n",
        "\n",
        "상태변수가 많은 태스크에 강화학습을 적용하기 위해 행동가치 함수를 표형식으로 나타내는 대신 층 수가 많은 신경망으로 바꾸어 나타낸다.\n",
        "\n",
        "각 상태변수의 값을 신경망의 입력으로 삼으며 신경망의 입력층이 갖는 유닛 수는 상태변수의 수와 같다. 역진자 태스크를 예로 들면, 상태는 위치 속도 각도 각속도 4개의 변수이므로 입력 유닛은 4개가 된다.\n",
        "\n",
        "***신경망에 상태변수를 입력할 때는 이산변수로 변환하는 과정이 필요없다.*** (세세한 구간을 요구하는 것이 아닌 pointwise 하게 변수로 받아들여질 수 있다.)\n",
        "\n",
        "출력층의 유닛 수는 행동의 가짓수와 같다. 역진자 태스크에서는 좌/우로 미는 행동 두 가지 이다.\n",
        "\n",
        "출력층 유닛의 출력값은 행동가치 함수의 값이다. 해당 행동을 취했을 때 받을 수 있으리라 예상되는 할인총보상값을 출력하는 것이다. 그리고 출력층 각 유닛이 출력하는 할인총보상값을 비교해보고 행동을 결정한다. 이는 곧 회귀문제에 해당된다.\n",
        "\n",
        "$Q$ 러닝 알고리즘은 오차 역전파 계산에 사용된다."
      ],
      "metadata": {
        "id": "aaupWY6jmMck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파이토치로 DQN 구현할 때 주의점\n",
        "\n",
        "파이토치로 DQN을 구현할 때 주의할 점 5가지가 있다.\n",
        "\n",
        "첫 번째는 Experience Replay 및 유사 Fixed Target Q-Network를 구현하기 위해 미니 배치 학습을 적용한다는 점이다. DQN은 각 단계의 transition을 메모리 객체에 저장한다. 여기서 transition은 [상태 $s_{t}$, 행동 $a_{t}$, 그 다음 상태 $s_{t+1}$, 그리고 보상 $r_{t+1}$]이 포함된다. 여기서 주의할 점은 봉이 쓰러지거나, 200단계를 버텨내면 게임이 종료되므로 그 다음 단계의 상태 $s_{t+1}$이 존재하지 않는다는 점이다.  이 때문에 다음 상태의 존재 여부에 따라 분기하도록 구현해야 한다.\n",
        "\n",
        "- 이번 장에서는 편의를 위해 Fixed Target Q-Network를 제대로 구현하는 target Q-Network를 사용하지 않고 그대로 main-network를 사용한다.\n",
        "\n",
        "두 번째는 파이토치에서 미니배치를 다루는 방법이다.\n",
        "\n",
        "세 번째는 변수의 데이터 타입이다. CartPole은 NumPy 타입으로 변수를 다루는데, 파이토치에서는 Torch.Tensor 타입으로 된 텐서로 변수를 다룬다.\n",
        "\n",
        "네 번째는 변수의 크기다. 특히 Torch.Tensor의 크기(dimension)에 주의해야 한다.\n",
        "\n",
        "다섯 번째는 namedtuple을 사용한다는 점이다. namedtuple을 사용하면 CartPole에서 관측된 상태 변수 값에 이름을 붙여 저장해둘 수 있다."
      ],
      "metadata": {
        "id": "VhK2RaNyYY68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN 구현"
      ],
      "metadata": {
        "id": "t7LdXxSKBn17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuA8JwGTmL99",
        "outputId": "26012ab0-1e10-4107-b539-3948ba54a615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tr(name_a='이름A', value_b=100)\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "# 이 코드에서는 nmaedtuple 사용\n",
        "# namedtuple을 사용하면 키-값의 쌍 형태로 값을 저장할 수 있다.\n",
        "# 그리고 키를 필드명으로 값에 접근할 수 있어 편리하다.\n",
        "# https://docs.python.org/3/library/collections.html#collections.namedtuple\n",
        "# 아래는 사용 예다.\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "Tr = namedtuple('tr', ('name_a', 'value_b'))\n",
        "Tr_object = Tr('이름A', 100)\n",
        "\n",
        "print(Tr_object) # 출력 : tr(name_a='이름A', value_b=100)\n",
        "print(Tr_object.value_b) # 출력 : 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# namedtuple 생성\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# 상수 정의\n",
        "ENV = 'CartPole-v0'\n",
        "GAMMA = 0.99 # 시간할인율\n",
        "MAX_STEPS = 200 # 1 에피소드 당 최대 단계 수\n",
        "NUM_EPISODES = 500 # 최대 에피소드 수"
      ],
      "metadata": {
        "id": "UEI9c-J2EeNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "미니배치 학습에서 경험 데이터를 저장하는 역할을 할 ReplayMemory 클래스를 정의한다. ReplayMemory 클래스는 각 단계에서 해당 단계의 transition을 저장하는 push 메서드와 무작위로 선택된 transition을 꺼내오는 sample 메서드를 갖추고 있다.\n",
        "\n",
        "이 클래스는 저장된 transition의 개수가 CAPACITY개를 초과하면 오래된 것부터 지우고 지워진 transition의 인덱스를 새로운 transition에 부여한다."
      ],
      "metadata": {
        "id": "Ix1Wf1tEGjjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transition을 저장하기 위한 메모리 클래스\n",
        "\n",
        "class ReplayMemory:\n",
        "\n",
        "  def __init__(self, CAPACITY):\n",
        "    self.capacity = CAPACITY # 메모리의 최대 저장 건수\n",
        "    self.memory = [] # 실제 transition을 저장할 변수\n",
        "    self.index = 0 # 저장 위치를 가리킬 인덱스 변수\n",
        "\n",
        "  def push(self, state, action, state_next, reward):\n",
        "    '''transition = (state, action, state_next, reward)을 메모리에 저장'''\n",
        "\n",
        "    if len(self.memory) < self.capacity:\n",
        "      self.memory.append(None) # 메모리가 가득 차지 않은 경우\n",
        "\n",
        "    # Transition이라는 namedtuple을 사용해 키-값 쌍의 형태로 값을 저장\n",
        "    self.memory[self.index] = Transition(state, action, state_next, reward)\n",
        "\n",
        "    self.index = (self.index + 1) % self.capacity # 다음 저장할 위치를 한 자리 뒤로 수정\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    '''batch_size 개수만큼 무작위로 저장된 transition을 추출'''\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    '''len 함수로 현재 저장된 transition 개수를 반환'''\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "GwrXCCEaIj-G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로 Brain 클래스를 구현한다. 이 클래스가 DQN 구현의 핵심이 되는 부분이다. 3장에서 구현했던 Q러닝에서는 Brain 클래스가 표 형식으로 나타낸 Q함수를 저장했다. 이번에는 표 대신 신경망으로 Q 함수를 나타낼 것이다. 이 클래스의 메서드는 replay와 decide_action의 2개다. replay 메서드는 메모리 클래스에서 미니배치를 꺼내와서 신경망의 결합 가중치를 학습하는 방법으로 Q 함수를 수정한다.\n",
        "\n",
        "decide_action 메서드는 $\\epsilon$-greedy 알고리즘으로 무작위로 선택된 행동 혹은 현재 상태에서 Q값이 최대가 되는 행동을 선택해서 그 행동의 인덱스 값을 반환한다."
      ],
      "metadata": {
        "id": "tTwbjORDNpAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 에이전트의 두뇌 역할을 하는 클래스. DQN을 실제 수행한다.\n",
        "# Q 함수를 딥러닝 신경망 형태로 정의한다.\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "CAPACITY = 10000\n",
        "\n",
        "class Brain:\n",
        "  def __init__(self, num_states, num_actions):\n",
        "    self.num_actions = num_actions # 행동의 가짓수 (왼쪽, 오른쪽)을 구함\n",
        "\n",
        "    # transition을 기억하기 위한 메모리 객체 생성\n",
        "    self.memory = ReplayMemory(CAPACITY)\n",
        "\n",
        "    # 신경망 구축\n",
        "    self.model = nn.Sequential()\n",
        "    self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
        "    self.model.add_module('relu1', nn.ReLU())\n",
        "    self.model.add_module('fc2', nn.Linear(32, 32))\n",
        "    self.model.add_module('relu2', nn.ReLU())\n",
        "    self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
        "\n",
        "    print(self.model) # 신경망 구조 출력\n",
        "\n",
        "    # 최적화 기법 선택\n",
        "\n",
        "    self.opitmizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "\n",
        "  def replay(self):\n",
        "    '''Experience Replay로 신경망의 결합 가중치 학습'''\n",
        "\n",
        "    # -------------------------------------\n",
        "    # 1. 저장된 transition 수 확인\n",
        "    # -------------------------------------\n",
        "    # 1.1. 저장된 transition의 수가 미니배치 크기보다 작으면 아무것도 하지 않음\n",
        "\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    # -------------------------------------\n",
        "    # 2. 미니배치 생성\n",
        "    # -------------------------------------\n",
        "    # 2.1. 메모리 객체에서 미니배치를 추출\n",
        "    transitions = self.memory.sample(BATCH_SIZE)\n",
        "\n",
        "    # 2.2. 각 변수를 미니배치에 맞는 형태로 변형\n",
        "    # transitions는 각 단계 별로 (state, action, state_next, reward) 형태로 BATCH_SIZE\n",
        "    # 개수만큼 저장됨\n",
        "    # 다시 말해 (state, action, state_next, reward) * BATCH_SIZE 형태가 된다.\n",
        "    # 이를 미니배치로 만들기 위해\n",
        "    # (state*BATCH_SIZE, action*BATCH_SIZE, state_next*BATCH_SIZE, reward*BATCH_SIZE)\n",
        "    # 형태로 변환한다\n",
        "    batch = Transition(*zip(*transitions)) ## parameter 를 몇 개 받을 지 모르기 때문\n",
        "\n",
        "    # 2.3. 각 변수의 요소를 미니배치에 맞게 변형하고, 신경망으로 다룰 수 있게 Variable로 만든다.\n",
        "    # state를 예로 들면, [torch.FloatTensor of size 1*4] 형태의 요소가 BATCH_SIZE\n",
        "    # 개수만큼 있는 형태다.\n",
        "    # 이를 torch.FloatTensor of size BATCH_SIZE*4 형태로 변형한다.\n",
        "    # 상태, 행동, 보상, non_final 상태로 된 미니배치를 나타내는 Variable을 생성\n",
        "    # cat 은 Concatenates(연접)을 의미\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                       if s is not None])\n",
        "\n",
        "\n",
        "    # -------------------------------------\n",
        "    # 3. 정답신호로 사용할 Q(s_t, a_t)를 계산\n",
        "    # -------------------------------------\n",
        "    # 3.1. 신경망을 추론 모드로 전환\n",
        "    self.model.eval()\n",
        "\n",
        "    # 3.2. 신경망으로 Q(s_t, a_t)를 계산\n",
        "    # self.model(state_batch)은 왼쪽, 오른쪽에 대한 Q값을 출력하며\n",
        "    # [torch.FloatTensor of size BATCH_SIZEx2] 형태다.\n",
        "    # 여기서부터는 실행한 행동 a_t에 대한 Q값을 계산하므로 action_batch에서 취한 행동\n",
        "    # a_t 가 왼쪽이냐 오른쪽이냐에 대한 인덱스를 구하고, 이에 대한 Q값을 gather 메서드로 모아온다.\n",
        "    state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # 3.3 max{Q(s_t+1, a)} 값을 계산한다. 이때 다음 상태가 존재하는지 주의해야 한다.\n",
        "\n",
        "    # cartpole이 done 상태가 아니고, next_state가 존재하는지 확인하는 인덱스 마스크를 만듬\n",
        "    non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
        "\n",
        "    # 먼저 전체를 0으로 초기화\n",
        "    next_state_values = torch.zeros(BATCH_SIZE)\n",
        "\n",
        "    # 다음 상태가 있는 인덱스에 대한 최대 Q값을 구한다.\n",
        "    # 출력에 접근해서 열방향 최댓값(max(1))이 되는 [값, 인덱스]를 구한다.\n",
        "    # 그리고 이 Q값(인덱스=0)을 출력한 다음\n",
        "    # detach 메서드로 이 값을 꺼내온다.\n",
        "    next_state_values[non_final_mask] = self.model(\n",
        "        non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "    # 3.4. 정답신호로 사용할 Q(s_t, a_t) 값을 Q러닝 식으로 계산한다.\n",
        "\n",
        "\n",
        "    # -------------------------------------\n",
        ""
      ],
      "metadata": {
        "id": "RmKTAwEBMTeq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CartPole 태스크의 에이전트 클래스. 봉 달린 수레 자체라고 보면 된다\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        '''태스크의 상태 및 행동의 가짓수를 설정'''\n",
        "        self.brain = Brain(num_states, num_actions)  # 에이전트의 행동을 결정할 두뇌 역할 객체를 생성\n",
        "\n",
        "    def update_q_function(self):\n",
        "        '''Q함수를 수정'''\n",
        "        self.brain.replay()\n",
        "\n",
        "    def get_action(self, state, episode):\n",
        "        '''행동을 결정'''\n",
        "        action = self.brain.decide_action(state, episode)\n",
        "        return action\n",
        "\n",
        "    def memorize(self, state, action, state_next, reward):\n",
        "        '''memory 객체에 state, action, state_next, reward 내용을 저장'''\n",
        "        self.brain.memory.push(state, action, state_next, reward)"
      ],
      "metadata": {
        "id": "lBWBMpHLSyqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CartPole을 실행하는 환경 역할을 하는 클래스\n",
        "\n",
        "\n",
        "class Environment:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV)  # 태스크를 설정\n",
        "        num_states = self.env.observation_space.shape[0]  # 태스크의 상태 변수 수(4)를 받아옴\n",
        "        num_actions = self.env.action_space.n  # 태스크의 행동 가짓수(2)를 받아옴\n",
        "        self.agent = Agent(num_states, num_actions)  # 에이전트 역할을 할 객체를 생성\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        '''실행'''\n",
        "        episode_10_list = np.zeros(10)  # 최근 10에피소드 동안 버틴 단계 수를 저장함(평균 단계 수를 출력할 때 사용)\n",
        "        complete_episodes = 0  # 현재까지 195단계를 버틴 에피소드 수\n",
        "        episode_final = False  # 마지막 에피소드 여부\n",
        "        frames = []  # 애니메이션을 만들기 위해 마지막 에피소드의 프레임을 저장할 배열\n",
        "\n",
        "        for episode in range(NUM_EPISODES):  # 최대 에피소드 수만큼 반복\n",
        "            observation = self.env.reset()  # 환경 초기화\n",
        "\n",
        "            state = observation  # 관측을 변환없이 그대로 상태 s로 사용\n",
        "            state = torch.from_numpy(state).type(\n",
        "                torch.FloatTensor)  # NumPy 변수를 파이토치 텐서로 변환\n",
        "            state = torch.unsqueeze(state, 0)  # size 4를 size 1*4로 변환\n",
        "\n",
        "            for step in range(MAX_STEPS):  # 1 에피소드에 해당하는 반복문\n",
        "\n",
        "                if episode_final is True:  # 마지막 에피소드에서는 각 시각의 이미지를 frames에 저장한다\n",
        "                    frames.append(self.env.render(mode='rgb_array'))\n",
        "\n",
        "                action = self.agent.get_action(state, episode)  # 다음 행동을 결정\n",
        "\n",
        "                # 행동 a_t를 실행하여 다음 상태 s_{t+1}과 done 플래그 값을 결정\n",
        "                # action에 .item()을 호출하여 행동 내용을 구함\n",
        "                observation_next, _, done, _ = self.env.step(\n",
        "                    action.item())  # reward와 info는 사용하지 않으므로 _로 처리\n",
        "\n",
        "                # 보상을 부여하고 episode의 종료 판정 및 state_next를 설정한다\n",
        "                if done:  # 단계 수가 200을 넘었거나 봉이 일정 각도 이상 기울면 done이 True가 됨\n",
        "                    state_next = None  # 다음 상태가 없으므로 None으로\n",
        "\n",
        "                    # 최근 10 에피소드에서 버틴 단계 수를 리스트에 저장\n",
        "                    episode_10_list = np.hstack(\n",
        "                        (episode_10_list[1:], step + 1))\n",
        "\n",
        "                    if step < 195:\n",
        "                        reward = torch.FloatTensor(\n",
        "                            [-1.0])  # 도중에 봉이 쓰러졌다면 페널티로 보상 -1을 부여\n",
        "                        complete_episodes = 0  # 연속 성공 에피소드 기록을 초기화\n",
        "                    else:\n",
        "                        reward = torch.FloatTensor([1.0])  # 봉이 서 있는 채로 에피소드를 마쳤다면 보상 1 부여\n",
        "                        complete_episodes = complete_episodes + 1  # 연속 성공 에피소드 기록을 갱신\n",
        "                else:\n",
        "                    reward = torch.FloatTensor([0.0])  # 그 외의 경우는 보상 0을 부여\n",
        "                    state_next = observation_next  # 관측 결과를 그대로 상태로 사용\n",
        "                    state_next = torch.from_numpy(state_next).type(\n",
        "                        torch.FloatTensor)  # numpy 변수를 파이토치 텐서로 변환\n",
        "                    state_next = torch.unsqueeze(state_next, 0)  # size 4를 size 1*4로 변환\n",
        "\n",
        "                # 메모리에 경험을 저장\n",
        "                self.agent.memorize(state, action, state_next, reward)\n",
        "\n",
        "                # Experience Replay로 Q함수를 수정\n",
        "                self.agent.update_q_function()\n",
        "\n",
        "                # 관측 결과를 업데이트\n",
        "                state = state_next\n",
        "\n",
        "                # 에피소드 종료 처리\n",
        "                if done:\n",
        "                    print('%d Episode: Finished after %d steps：최근 10 에피소드의 평균 단계 수 = %.1lf' % (\n",
        "                        episode, step + 1, episode_10_list.mean()))\n",
        "                    break\n",
        "\n",
        "            if episode_final is True:\n",
        "                # 애니메이션 생성 및 저장\n",
        "                display_frames_as_gif(frames)\n",
        "                break\n",
        "\n",
        "            # 10 에피소드 연속으로 195단계를 버티면 태스크 성공\n",
        "            if complete_episodes >= 10:\n",
        "                print('10 에피소드 연속 성공')\n",
        "                episode_final = True  # 다음 에피소드에서 애니메이션을 생성"
      ],
      "metadata": {
        "id": "Z8UazzxnZn7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행 엔트리 포인트\n",
        "cartpole_env = Environment()\n",
        "cartpole_env.run()"
      ],
      "metadata": {
        "id": "s3jK2mLRZqvW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}