{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/pytorch-drl/blob/main/DRL_Chap3_2_CartPole_Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'PyTorch를 활용한 강화학습/심층강화학습 실전입문' 책 스터디 내용을 Google Colab으로 정리하여 올립니다.\n",
        "\n",
        "Github 예제 코드 주소 : 'https://github.com/wikibook/pytorch-drl'\n",
        "\n",
        "PyTorch를 활용한 강화학습/심층강화학습 실전입문\n",
        "\n",
        "https://wikibook.co.kr/pytorch-drl/"
      ],
      "metadata": {
        "id": "_urokx68V926"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***가상환경 구현 및 함수 수정***"
      ],
      "metadata": {
        "id": "AJTGfJ9a0Spf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matplotlib -downgrade\n",
        "\n",
        "!pip install matplotlib==3.4.2"
      ],
      "metadata": {
        "id": "vt4pA63J0hb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install -y xvfb #server install\n",
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "  !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "  %pip install -U tf-agents pyvirtualdisplay\n",
        "  %pip install -U gym>=0.21.0\n",
        "  %pip install -U gym[box2d,atari, accept-rom-license]\n",
        "\n",
        "!xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook"
      ],
      "metadata": {
        "id": "3laxoCpM0hh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***display_frames_as_gif 함수 정의***"
      ],
      "metadata": {
        "id": "lM2T5YjW0uoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구현에 사용할 패키지 임포트\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gym"
      ],
      "metadata": {
        "id": "4yVtD8BU0881"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 애니메이션을 만드는 함수\n",
        "# 참고 URL : https://github.com/patrickmineault/xcorr-notebooks/blob/master/notebooks/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
        "!pip install JSAnimation\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import display\n",
        "\n",
        "#새로 정의한 display_animtaion 함수\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_animation(anim):\n",
        "  return HTML(anim.to_jshtml())"
      ],
      "metadata": {
        "id": "qllUOfzc1O8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_frames_as_gif(frames):\n",
        "  \"\"\"\n",
        "  Displays a list of frames as a gif, with controls\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(frames[0].shape[1]/48.0, frames[0].shape[0]/48.0), dpi=72)\n",
        "  patch = plt.imshow(frames[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=20)\n",
        "\n",
        "  anim.save('movie_cartpole.mp4') #애니메이션을 저장하는 부분\n",
        "  display(display_animation(anim)) #수정된 부분"
      ],
      "metadata": {
        "id": "GCZDAaTS1a3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 다변수, 연속값 상태를 표형식으로 나타내기"
      ],
      "metadata": {
        "id": "Z3Xfj4q1aPS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CartPole 의 상태\n",
        "\n",
        "이전 장에서 다뤘던 미로 태스크에서 상태는 에이전트가 어느 칸에 위치했는지를 변수 하나로 나타냈으며 0~8의 단순 이산값이었다. 그러나 역진자 태스크에서는 더 복잡하게 상태가 정의된다.\n",
        "\n",
        "CartPole 태스크는 observation 변수에 상태를 저장한다. env.step(action)은 게임 환경을 1단계 진행시키는 명령어로 action = 0 일 때 수레를 왼쪽으로 움직이는 것이고 1 일 때 오른쪽으로 움직이는 것에 해당한다.\n",
        "\n",
        "env.step(action) 은 observation, reward, done, info 라는 4개의 변수를 반환한다. observation은 수레와 봉의 상태를 나타내며 상태는 다시 4개의 변수로 이루어져 있다. 이는 수레의 위치 (-2.4 ~ 2.4), 수레의 속도 (-inf ~ inf), 봉의 각도 (-41.8도 ~ 41.8), 봉의 가속도 (-inf ~ inf)이다.\n",
        "\n",
        "reward는 즉각보상이며 action을 실행한 후에 수레의 위치가 $\\pm2.4$ 범위 안에 있고 봉이 $20.9$도 이상 기울어 있지 않다면 보상 1을 받는다. 반대의 경우, 보상은 0이 된다.\n",
        "\n",
        "done 은 게임 종료 여부를 나타내며, 200단계를 초과하거나, 보상이 0일 될 때 게임이 종료되어 done 의 값이 True가 된다.\n",
        "\n",
        "info 는 디버깅 등에 필요한 정보를 담은 변수이다."
      ],
      "metadata": {
        "id": "ia9hsu7NSnFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "observation의 4가지 변수는 모두 연속값으로 이루어져있다. 이를 표형식으로 나타내기 위해 연속값을 이산값으로 변환해야 한다.\n",
        "\n",
        "예를 들어 수레의 위치를 6개의 구간의 이산값으로 변환한다면 -2.4 / -1.6 / -0.8 / 0 / 0.8 / 1.6 / 2.4 로 구간을 나누어 0 ~ 5의 값을 반환할 수 있다. 그러나 이 양 끝 범위를 벗어날 가능성도 있으므로 -Inf ~ -1.6 을 0으로, 1.6 ~ Inf 를 5로 정의한다.\n",
        "\n",
        "다른 변수도 6개의 구간을 갖는 이산변수로 변환한다. 그러면 변수의 가짓수가 $6^{4} = 1296$개로 수레의 상태를 나타낼 수 있다.\n",
        "\n",
        "CartPole 에서 취할 수 있는 행동은 수레를 오른쪽이나 왼쪽으로 밀기 두 가지다. 이후 수레는 가속도를 부여받는다. 각 상태변수를 6개의 구간을 갖는 이산변수로 변환하면 CartPole의 Q함수는 1296행 2열로 된 표로 나타낼 수 있다."
      ],
      "metadata": {
        "id": "rLmhVOJVNMBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 상태의 이산변수 변환 구현\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "ENV = 'CartPole-v0' # 태스크 이름\n",
        "NUM_DIZITIZED = 6 # 각 상태를 이산변수로 변환할 구간 수\n",
        "\n",
        "env = gym.make(ENV) # 실행할 태스크 설정\n",
        "observation = env.reset() # 환경 초기화\n",
        "```\n",
        "\n",
        "env.reset()을 호출하면 초기 상태가 반환되므로 이 반환값을 observation 에 저장한다. 그 다음 이 변수의 값을 이산변수로 변환하는 함수를 정의한다.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# 이산값으로 만들 구간 계산\n",
        "def bins(climp_min, climp_max, num):\n",
        "  '''\n",
        "  관측된 상태(연속값)을 이산값으로 변환하는 구간을 계산\n",
        "  '''\n",
        "  return np.linspace(climp_min, climp_max, num+1)[1:-1]\n",
        "\n",
        "def digitize_state(observation):\n",
        "  '''\n",
        "  관측된 상태(observation 변수)를 이산값으로 변환\n",
        "  '''\n",
        "  cart_pos, cart_v, pole_angle, pole_v = observation\n",
        "\n",
        "  digitized = [\n",
        "    np.digitize(cart_post, bins=bins(-2.4, 2.4, NUM_DIGITIZED)),\n",
        "    np.digitize(cart_v, bins=bins(-3.0, 3.0, NUM_DIGITIZED)),\n",
        "    np.digitize(pole_angle, bins=bins(-0.5, 0.5, NUM_DIGITIZED)),\n",
        "    np.digitize(pole_v, bins=bins(-2.0, 2.0, NUM_DIGITIZED))]\n",
        "\n",
        "  return sum([x * (NUM_DIGITIZED ** i) for i, x in enumerate(digitized)])\n",
        "    \n",
        "```\n",
        "\n",
        "pole_angle 변수의 구간을 -0.5 ~ 0.5 까지 설정한 이유는 단위가 라디안 radian 이기 때문이다.\n",
        "\n",
        "$0.5rad  = 0.5 \\cdot \\frac{180}{\\pi} \\approx 28.65$  약 29도가 된다.\n",
        "\n",
        "digitize_state 함수의 반환값은 이산값으로 변환한 상태를 나타내는 4개의 변수를 모두 합쳐 0 부터 1295 사이의 값으로 변환한 것이다. NUM_DIGITIZED = 6 이라면 6진수로 계산한다. 만약 digitized = [[1],[2],[3],[4]] 라면 다음과 같이 변환할 수 있다.\n",
        "\n",
        "$$ 1\\cdot 6^{0} + 2\\cdot 6^{1} + 3\\cdot 6^{2} + 4\\cdot 6^{3} = 985$$\n",
        "이는 digitized  = $(\\mathbb{Z}_{6})^{4}$ 와 $(0, 1295) \\subset \\mathbb{Z}$ 두 집합 간의 ***bijection***이 존재하기 때문에 가능하다"
      ],
      "metadata": {
        "id": "pxYB8yMcPNy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum([x*(6**i) for i, x in enumerate([np.array([5]),np.array([5]), np.array([5]), np.array([5])])])"
      ],
      "metadata": {
        "id": "YLbDzq1jTnyn",
        "outputId": "ccd43929-8eb3-4544-c558-e24268d7f1c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1295])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(enumerate)\n",
        "\n",
        "#  enumerate is useful for obtaining an indexed list:\n",
        "#        (0, seq[0]), (1, seq[1]), (2, seq[2]), ..."
      ],
      "metadata": {
        "id": "N33fCtic1lfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(np.digitize)\n",
        "# digitize(x, bins, right=False)\n",
        "#     Return the indices of the bins to which each value in input array belongs.\n",
        "\n",
        "#     =========  =============  ============================\n",
        "#     `right`    order of bins  returned index `i` satisfies\n",
        "#     =========  =============  ============================\n",
        "#     ``False``  increasing     ``bins[i-1] <= x < bins[i]``\n",
        "#     ``True``   increasing     ``bins[i-1] < x <= bins[i]``\n",
        "#     ``False``  decreasing     ``bins[i-1] > x >= bins[i]``\n",
        "#     ``True``   decreasing     ``bins[i-1] >= x > bins[i]``\n",
        "#     =========  =============  ============================\n",
        "\n",
        "#     If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is\n",
        "#     returned as appropriate."
      ],
      "metadata": {
        "id": "lYekuM9iR4jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV = 'CartPole-v0' # 태스크 이름\n",
        "NUM_DIZITIZED = 6 # 각 상태를 이산변수로 변환할 구간 수\n",
        "\n",
        "env = gym.make(ENV) # 실행할 태스크 설정\n",
        "observation = env.reset() # 환경 초기화"
      ],
      "metadata": {
        "id": "dS9F3iTacqJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이산값으로 만들 구간 계산\n",
        "def bins(climp_min, climp_max, num):\n",
        "  '''\n",
        "  관측된 상태(연속값)을 이산값으로 변환하는 구간을 계산\n",
        "  '''\n",
        "  return np.linspace(climp_min, climp_max, num+1)[1:-1]\n",
        "\n",
        "def digitize_state(observation):\n",
        "  '''\n",
        "  관측된 상태(observation 변수)를 이산값으로 변환\n",
        "  '''\n",
        "  cart_pos, cart_v, pole_angle, pole_v = observation\n",
        "\n",
        "  digitized = [\n",
        "    np.digitize(cart_post, bins=bins(-2.4, 2.4, NUM_DIGITIZED)),\n",
        "    np.digitize(cart_v, bins=bins(-3.0, 3.0, NUM_DIGITIZED)),\n",
        "    np.digitize(pole_angle, bins=bins(-0.5, 0.5, NUM_DIGITIZED)),\n",
        "    np.digitize(pole_v, bins=bins(-2.0, 2.0, NUM_DIGITIZED))]\n",
        "\n",
        "  return sum([x * (NUM_DIGITIZED ** i) for i, x in enumerate(digitized)])"
      ],
      "metadata": {
        "id": "MnxG_0AocvF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q러닝 구현"
      ],
      "metadata": {
        "id": "Sv-8pmBZcj1F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oJAAC0X9ZMie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}